{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvvishnu/CS6910-Assignments/blob/main/Assignment2/partA/Assignment2_partA_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6y6344Ixh9y",
        "outputId": "dc515940-9571-473d-fe51-9eb20335e8dd"
      },
      "source": [
        "from tensorflow.keras import *\n",
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/5d/20ab24504de2669c9a76a50c9bdaeb44a440b0e5e4b92be881ed323857b1/wandb-0.10.26-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.7MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 27.5MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=0c097f2efd709e23b6e2ee7a4da853ebf142eefb3b88d24d032bc854cd38610d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=0130dfe218ff42edc4576909097bd9f5225ec71fa3c127a257f3541110b4a7c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: shortuuid, sentry-sdk, smmap, gitdb, GitPython, docker-pycreds, configparser, pathtools, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrbWkZtvy4RI",
        "outputId": "fbe742af-6f15-4a59-a32e-fb8c7171871d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gUIvbhpzad6"
      },
      "source": [
        "# Splitting training data into training and validation set in the ratio 9:1 \n",
        "\n",
        "# # import os\n",
        "# # path = '/content/drive/MyDrive/nature_12K.zip'\n",
        "# # import zipfile\n",
        "# # with zipfile.ZipFile(path, 'rw') as zip_ref:\n",
        "# #     zip_ref.extractall('/content/drive/MyDrive/Assignment2')\n",
        "# !pip install split-folders tqdm\n",
        "# import splitfolders\n",
        "# splitfolders.ratio(path+'train', output=path+'split', seed=1337, ratio=(.9, .1), group_prefix=None) # default values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJbgq06Zy9qg"
      },
      "source": [
        "path = '/content/drive/MyDrive/Assignment2/inaturalist_12K/' #Load the inaturalist_12K dataset\n",
        "\n",
        "##simple function which returns the CNN consisting of 5 convolution layers\n",
        "#filters is a list , where filters[i] indicates num of filters in ith convolutional layer (0 indexing)\n",
        "#kern_filter is a list of tuples denoting the kernel_size of the filter\n",
        "#actiavation_fn is a list of activation functions\n",
        "\n",
        "def simplenetwork(filters,kern_filter,activation_fn,dense_layer_size):\n",
        "    model = Sequential()\n",
        "    for i in range(5):\n",
        "          if(i == 0):\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],input_shape=(224,224,3),activation=activation_fn[i]))     \n",
        "          else :\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],activation=activation_fn[i]))\n",
        "          model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(dense_layer_size,activation=activation_fn[5]))\n",
        "    model.add(layers.Dense(10,activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzDstdrvzBiZ",
        "outputId": "31dc5ca5-0849-4ea0-8a3f-f975f4462d03"
      },
      "source": [
        "model = simplenetwork([32,32,32,32,32],[(3,3),(3,3),(3,3),(3,3),(3,3)],['relu' for i in range(6)],1024)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 52, 52, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 10, 10, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              820224    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 868,362\n",
            "Trainable params: 868,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9MNV5uvzERx"
      },
      "source": [
        "#More general function where we can add drop_out batch_normalisation , avg_pool etc.\n",
        "#filters is a list , where filters[i] indicates num of filters in ith convolutional layer (0 indexing)\n",
        "#kern_filter is a list of tuples denoting the kernel_size of the filter\n",
        "#actiavation_fn is a list of activation functions\n",
        "#drop_out is the value of drop_out we will apply\n",
        "#batch_normalisation if set true we do batch normalisation after each layer\n",
        "#dense_layer is the size of dense layer\n",
        "#train is train data\n",
        "#validation is validation data\n",
        "#num_classes is the number of output classes , this case we pass 10\n",
        "#epochs is number of epochs\n",
        "#drop_batch_dense , if 'both' then we apply dropout followed by batchnormalisation after dense layer , if set 'batch' we do only batch_normalisation after dense layer and if ser 'drop' we apply dropout after dense layer\n",
        "#last2avg_pool if set true then for last2 Convolutional layers we do avgpool instead of maxpooling\n",
        "#dp_after_each CNN , if set true then we apply dropout after every ConV layer else we apply dropout only on last2 convolutional layers\n",
        "\n",
        "def network(filters,kern_filter,activation_fn,drop_out,batch_normalisation,dense_layer,train,validation,num_classes,epochs,drop_batch_dense,last_2_avgpool,dp_aft_eachCNN):\n",
        "    \n",
        "     model = Sequential()\n",
        "     \n",
        "     if (batch_normalisation):\n",
        "        for i in range(5):\n",
        "          if(i == 0):\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],input_shape=(224,224,3),activation=activation_fn[i]))     \n",
        "          else :\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],activation=activation_fn[i]))\n",
        "\n",
        "          if (i<=2 or not(last_2_avgpool)):\n",
        "             if(dp_aft_eachCNN or i>2):\n",
        "                model.add(layers.Dropout(drop_out))\n",
        "             model.add(layers.BatchNormalization())\n",
        "             model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "          else:\n",
        "             model.add(layers.Dropout(drop_out))\n",
        "             model.add(layers.BatchNormalization())\n",
        "             model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "     else:\n",
        "        for i in range(5):\n",
        "          if(i == 0):\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],input_shape=(224,224,3),activation=activation_fn[i]))\n",
        "          else :\n",
        "            model.add(layers.Conv2D(filters[i],kern_filter[i],activation=activation_fn[i]))\n",
        "          if(i<=2 or not(last_2_avgpool)):\n",
        "             if(i>2 or dp_aft_eachCNN):\n",
        "               model.add(layers.Dropout(drop_out))\n",
        "             model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "          else:\n",
        "             model.add(layers.Dropout(drop_out))\n",
        "             model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "     model.add(layers.Flatten())\n",
        "     model.add(layers.Dropout(drop_out))\n",
        "     model.add(layers.BatchNormalization())\n",
        "     model.add(layers.Dense(dense_layer,activation='relu'))  #activation_fn[5]))\n",
        "     if(drop_batch_dense == 'both' or drop_batch_dense == 'drop'):\n",
        "         model.add(layers.Dropout(drop_out))\n",
        "     if(drop_batch_dense == 'both' or drop_batch_dense == 'batch'):\n",
        "         model.add(layers.BatchNormalization())\n",
        "     model.add(layers.Dense(num_classes))\n",
        "     model.add(layers.Softmax())\n",
        "     labels = ['Amphibia','Animalia','Arachnida','Aves','Fungi','Insecta','Mammalia','Mollusca','Plantae','Reptilia']\n",
        "     print(model.summary())\n",
        "\n",
        "     model.compile(optimizer='adam',loss=losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "     model.fit(train,epochs=epochs,validation_data=validation,callbacks=[WandbCallback()])\n",
        "\n",
        "     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b7XPA6O0cCc"
      },
      "source": [
        "#a function for Image processing , this returns generators for the train and validation_data\n",
        "#data_augmentation if True then we augment the training data\n",
        "#batch_size is the batch size of data\n",
        "def ImageProcessing(data_augmentation,batch_size):\n",
        "    \n",
        "      augmented = preprocessing.image.ImageDataGenerator(\n",
        "                              #featurewise_center=True,\n",
        "                              #featurewise_std_normalization=True,\n",
        "                              rotation_range=20,\n",
        "                              width_shift_range=0.2,\n",
        "                              height_shift_range=0.2,\n",
        "                              horizontal_flip=True)\n",
        "      datagen = preprocessing.image.ImageDataGenerator()\n",
        "      train = []\n",
        "      validation = []\n",
        "      if(data_augmentation):\n",
        "          train = augmented.flow_from_directory(path+'split/train/',target_size=(224,224),color_mode='rgb', classes=None,class_mode='categorical', batch_size=batch_size)\n",
        "      else:\n",
        "          train = datagen.flow_from_directory(path+'split/train/',target_size=(224,224),color_mode='rgb', classes=None,class_mode='categorical', batch_size=batch_size)\n",
        "      validation = datagen.flow_from_directory(path+'split/val/',target_size=(224,224),color_mode='rgb', classes=None,class_mode='categorical', batch_size=batch_size)\n",
        "\n",
        "      return (train,validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwbGWMFa0mzL"
      },
      "source": [
        "# A function which calls the network function , this function makes ready the train_data and validation data and call the networks function\n",
        "def CNN(filter,filter_organisation,drop_out,batch_normalisation,data_augmentation,kernel_size,batch_size,epochs,drop_batch_dense,last_2_avgpool,dense_sz,dp_aft_eachCNN):\n",
        "\n",
        "   (train,validation) = ImageProcessing(data_augmentation,batch_size)\n",
        "   filters = []\n",
        "   if(filter_organisation == 'same'):\n",
        "       filters = [filter for i in range(5)]\n",
        "   if(filter_organisation == 'half'):\n",
        "       filters = [filter//(2**i) for i in range(5)]\n",
        "   if(filter_organisation == 'double'):\n",
        "       filters = [filter*(2**i) for i in range(5)]\n",
        "\n",
        "   return network(filters,kernel_size,['relu' for i in range(6)],drop_out,batch_normalisation,dense_sz,train,validation,10,epochs,drop_batch_dense,last_2_avgpool,dp_aft_eachCNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgXcH0wP1Wxq"
      },
      "source": [
        "# A train_function which wandb calls\n",
        "def train():\n",
        "  config_default = { \n",
        "        'filter': 16,\n",
        "        'filter_organisation': 'double',\n",
        "        'drop_out':0.2,\n",
        "        'batch_normalisation': True,\n",
        "        'data_augmentation': True ,\n",
        "        'kernel_size': [(3,3),(3,3),(5,5),(7,7),(11,11)],\n",
        "        'batch_size': 32,\n",
        "        'epochs':10,\n",
        "        'drop_batch_dense':'both',\n",
        "        'last_2_avgpool': True,\n",
        "        'dp_aft_eachCNN' : False,\n",
        "  }\n",
        "  print(\"wand about to be initialised\")\n",
        "  wandb.init(config=config_default,project='Assignment2')\n",
        "  config = wandb.config\n",
        "\n",
        "  CNN(config.filter,config.filter_organisation,config.drop_out,config.batch_normalisation,config.data_augmentation,config.kernel_size,config.batch_size,config.epochs,config.drop_batch_dense,config.last_2_avgpool,config.dense_sz,config.dp_aft_eachCNN)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgAFoENM1b67"
      },
      "source": [
        "#sweep configuration\n",
        "sweep_config = {\n",
        "    'name': 'CNN',\n",
        "    'method' : 'bayes',\n",
        "    'metric': {        \n",
        "         'goal': 'maximize',\n",
        "         'name' : 'val_accuracy'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'filter':{\n",
        "            'values': [32,64]\n",
        "        },\n",
        "        'filter_organisation':{\n",
        "             'values':['double','same','half']\n",
        "        },\n",
        "        'drop_out':{\n",
        "             'values':[0.2,0.3]\n",
        "        },\n",
        "        'batch_normalisation':{\n",
        "            'values':[True,False]\n",
        "        },\n",
        "        'data_augmentation':{\n",
        "            'values':[True,False]\n",
        "        },\n",
        "        'kernel_size':\n",
        "        {\n",
        "            'values':[[(3,3),(3,3),(3,3),(3,3),(3,3)],[(3,3),(3,3),(5,5),(7,7),(7,7)],[(11,11),(5,5),(7,7),(3,3),(3,3)]]\n",
        "        },\n",
        "        'batch_size':\n",
        "        {\n",
        "            'values':[32,64]\n",
        "        },\n",
        "        'epochs':\n",
        "        {\n",
        "            'values':[25,30]\n",
        "        },\n",
        "        'drop_batch_dense':\n",
        "        {\n",
        "            'values' : ['both','drop','batch']\n",
        "        },\n",
        "        'last_2_avgpool':\n",
        "        {\n",
        "            'values':[True,False]\n",
        "        },\n",
        "        'dense_sz':\n",
        "        {\n",
        "            'values':[128,256,1024]\n",
        "        },\n",
        "        'dp_aft_eachCNN':\n",
        "        {\n",
        "            'values' : [True,False]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxhaIEmT1gTm"
      },
      "source": [
        "#sweep_id = wandb.sweep(sweep_config,entity=\"shivaram_22\",project=\"Assignment2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjIUnoPe1iwr"
      },
      "source": [
        "#wandb.agent(sweep_id,project=\"Assignment2\",function=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrSkweL71jzq"
      },
      "source": [
        "#CNN(config.filter,config.filter_organisation,config.drop_out,config.batch_normalisation,config.data_augmentation,config.kernel_size,config.batch_size,config.epochs,config.drop_batch_dense,config.last_2_avgpool,config.dense_sz,config.dp_aft_eachCNN)  \n",
        "#collecting the model of the best hyperparameter configuration\n",
        "model = CNN(32,'double',0.2,True,True,[(3,3),(3,3),(5,5),(7,7),(7,7)],64,25,'both',True,1024,False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjaI9Xk-2LRQ"
      },
      "source": [
        "#applying the model on test data\n",
        "datagen = preprocessing.image.ImageDataGenerator()\n",
        "print(path+'val/')\n",
        "test = validation = datagen.flow_from_directory(path+'val/',target_size=(224,224),color_mode='rgb', classes=None,class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ymsC7h2QfT"
      },
      "source": [
        "m = model.evaluate(test,return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVH_MQj02Th6"
      },
      "source": [
        "!pip install h5py\n",
        "#Saving the best model as json and h5 files\n",
        "from keras.models import model_from_json\n",
        "from tensorflow.keras import *\n",
        "model_path = '/content/drive/MyDrive/Assignment2/'\n",
        "# reffered this for saving and loading the model 'https://machinelearningmastery.com/save-load-keras-deep-learning-models/'\n",
        "\n",
        "# model_json = model.to_json()\n",
        "# with open(model_path+\"model.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# model.save_weights(model_path+\"model.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "\n",
        "#Loading the model from disk\n",
        "json_file = open(model_path+'model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(model_path+\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "loaded_model.summary()\n",
        "#recompiling the loaded model\n",
        "loaded_model.compile(optimizer='adam',loss=losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "model = loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}