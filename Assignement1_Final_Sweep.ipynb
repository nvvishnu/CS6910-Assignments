{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Assignement1_Final_Sweep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbf4ce3667d243689f2a3149a146fe76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47c9f9b4911a4d60a22a6a6f9d4f2422",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd1b93d89a71420c95886611f03e8ac4",
              "IPY_MODEL_de0e9bd86b124fd9a8e3934a0a52d440"
            ]
          }
        },
        "47c9f9b4911a4d60a22a6a6f9d4f2422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd1b93d89a71420c95886611f03e8ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_573bd6efba9c4f0faeabc09149be366c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d59b9ce1299431da8640288100bed8a"
          }
        },
        "de0e9bd86b124fd9a8e3934a0a52d440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f78b13f25165442d91354605acf682ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d292200ee4d64960b94fe165bbb6aa98"
          }
        },
        "573bd6efba9c4f0faeabc09149be366c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d59b9ce1299431da8640288100bed8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f78b13f25165442d91354605acf682ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d292200ee4d64960b94fe165bbb6aa98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2jhbImHGSyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6cac62e-7b6b-4bdf-8e13-d4367a44436e"
      },
      "source": [
        "!pip install wandb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import wandb\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot\r\n",
        "import math\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.22)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1OfVTIUo_g5"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\r\n",
        "trainX, validateX, trainy, validatey = train_test_split(trainX,trainy, test_size=0.1, random_state=1)\r\n",
        "\r\n",
        "trainX = trainX/255\r\n",
        "testX = testX/255\r\n",
        "validateX = validateX/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYlO_HwVjP2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "15053f5a-98f7-44e6-8361-222aa79538cd"
      },
      "source": [
        "!wandb login \r\n",
        "wandb.init(project=\"cs6910-assignment1\", entity=\"nvvishnu\")\r\n",
        "num_classes = max(max(trainy),max(testy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnvvishnu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnvvishnu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">dulcet-smoke-215</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/3r8op20f\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/3r8op20f</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_100124-3r8op20f</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MOAEoEsMQUR"
      },
      "source": [
        "def square_list(lst):\r\n",
        "    return [i ** 2 for i in lst]\r\n",
        "def logistic(z):\r\n",
        "\r\n",
        "  l = []\r\n",
        " # print(z.shape)\r\n",
        "  for i in z:\r\n",
        "     #print(type(i),i)\r\n",
        "     if (i[0]<-600):\r\n",
        "       l.append(0)\r\n",
        "     else:\r\n",
        "       l.append(1./(1+math.exp(-i[0])))\r\n",
        "  \r\n",
        "  t = np.array(l)\r\n",
        "\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "\r\n",
        "  return t\r\n",
        "def relu(z):\r\n",
        "  l=[]\r\n",
        "  for i in z:\r\n",
        "    l.append(max(0,i[0]))\r\n",
        "  t= np.array(l)\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "  return t\r\n",
        "def relu_grad(z):\r\n",
        "  l=[]\r\n",
        "  for i in z:\r\n",
        "    if(i[0]<0):\r\n",
        "      l.append(0)\r\n",
        "    else:\r\n",
        "      l.append(1)  \r\n",
        "  t= np.array(l)\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "  return t  \r\n",
        "\r\n",
        "def log_grad(z):\r\n",
        "    return logistic(z)*(1-logistic(z))\r\n",
        "def tanh(z):\r\n",
        "    l = []\r\n",
        "    for i in z:\r\n",
        "      if (i[0]>=600):\r\n",
        "        l.append(1)\r\n",
        "      elif (i[0]<=(-600)):\r\n",
        "        l.append(-1)\r\n",
        "      else:\r\n",
        "        l.append((math.exp(i[0])-math.exp(-i[0]))/((math.exp(i[0])+math.exp(-i[0]))))\r\n",
        "        \r\n",
        "    t = np.array(l)\r\n",
        "    t = t.reshape(t.shape[0],1)\r\n",
        "    return t\r\n",
        "\r\n",
        "def tanh_grad(z):\r\n",
        "\r\n",
        "  return 1-((tanh(z))*(tanh(z)))\r\n",
        "def cross_entropy_loss(X_test,Y_test,weights,biases):\r\n",
        "\r\n",
        "    sum = 0\r\n",
        "    for i in range(Y_test.shape[0]):\r\n",
        "      y_ = feed_forward(X_test[i],logistic,weights,biases)[2]\r\n",
        "      if(Y_test[i]<y_.shape[0]):\r\n",
        "        if(y_[Y_test[i]][0] != 0):\r\n",
        "           sum = sum - math.log(y_[Y_test[i]][0])\r\n",
        "        else:\r\n",
        "           print(\"very bad\\n\")\r\n",
        "           sum = sum + 100000\r\n",
        "\r\n",
        "    return sum/Y_test.shape[0]\r\n",
        "def accuracy(X_test,Y_test,weights,biases):\r\n",
        "    \r\n",
        "    sum = 0\r\n",
        "    cnt = 0\r\n",
        "\r\n",
        "    for i in range(Y_test.shape[0]):\r\n",
        "\r\n",
        "     \r\n",
        "      y_ = feed_forward(X_test[i],logistic,weights,biases)[2]\r\n",
        "      y_ = y_.flatten()\r\n",
        "      #print(y_)\r\n",
        "      j = np.argmax(y_)\r\n",
        "\r\n",
        "      if (Y_test[i] == j):\r\n",
        "        sum = sum+1\r\n",
        "\r\n",
        "    return sum/Y_test.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUfbRg8ptAsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599994b1-d999-42db-8dff-6ec537ff1637"
      },
      "source": [
        "# wandb.log({\"Dum\": wandb.Image(trainX[0])})\r\n",
        "\r\n",
        "# #  There are 10 labels in the dataset\r\n",
        "# labelNames = ['T-shirt/top', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\r\n",
        "\r\n",
        "# images=[]\r\n",
        "# image_arrays=[]\r\n",
        "# for i in range(0,num_classes+1):\r\n",
        "#   for j in range(0,trainX.shape[0]):\r\n",
        "#     if(trainy[j]==i):\r\n",
        "#       #print(i)\r\n",
        "#       pyplot.subplot(1,1,(1,1+i))\r\n",
        "#       pyplot.title(labelNames[i])\r\n",
        "#       # plot raw pixel data\r\n",
        "#       (pyplot.imshow(trainX[j], cmap=pyplot.get_cmap('gray'),))     \r\n",
        "#       image_arrays.append(trainX[j])\r\n",
        "#       # show the figure\r\n",
        "#       pyplot.show()\r\n",
        "#       break\r\n",
        "# print(type(pyplot))\r\n",
        "# # wandb.log({\"Question 1\": pyplot})\r\n",
        "# # to_plot=np.array(image_arrays)\r\n",
        "# # to_plot=to_plot.reshape((10,28,28,1))\r\n",
        "# # print(type(to_plot))\r\n",
        "# # print(to_plot.shape)\r\n",
        "# # print(type(to_plot[0]))\r\n",
        "# # # for i in range(to_plot.shape[0]):\r\n",
        "# # #    image=to_plot[i]\r\n",
        "# # #    print(image)\r\n",
        "# # #    print(type(image))\r\n",
        "# # # wandb.log({\"Question 1\": [wandb.Image(image)]})\r\n",
        "# # wandb.log({\"Question 1\": [wandb.Image(to_plot[i], caption=\"Label\")]})\r\n",
        "# # wandb.log({\"Question 1\": to_plot[0]})\r\n",
        "# # [images[0],images[1],images[2],images[3],images[4],images[5],images[6],images[7],images[8],images[9]\r\n",
        "# #Yet to implement how to log this into wandb\r\n",
        "\r\n",
        "# #  There are 10 labels in the dataset\r\n",
        "# labelNames = ['T-shirt/top', 'Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\r\n",
        "\r\n",
        "# image_arrays=[]\r\n",
        "# for i in range(0,num_classes+1):\r\n",
        "#   for j in range(0,trainX.shape[0]):\r\n",
        "#     if(trainy[j]==i):  \r\n",
        "#       print(trainX[j])\r\n",
        "#       # wandb.log({\"Question 1\":wandb.Image(trainX[j],caption=labelNames[i])})\r\n",
        "#       break\r\n",
        "# # to_plot=np.array(image_arrays)\r\n",
        "# # to_plot=to_plot.reshape((10,28,28,1))\r\n",
        "# # print(type(to_plot))\r\n",
        "# # print(to_plot.shape)\r\n",
        "# # print(type(to_plot[0]))\r\n",
        "# # for i in range(to_plot.shape[0]):\r\n",
        "# #    image=to_plot[i]\r\n",
        "# #    print(image)\r\n",
        "# #    print(type(image))\r\n",
        "# # wandb.log({\"Question 1\": [wandb.Image(image)]})\r\n",
        "# wandb.log({\"Question 1\": [wandb.Image(to_plot[i], caption=\"Label\")]})\r\n",
        "# # wandb.log({\"Question 1\": to_plot[0]})\r\n",
        "# # [images[0],images[1],images[2],images[3],images[4],images[5],images[6],images[7],images[8],images[9]\r\n",
        "# #Yet to implement how to log this into wandb\r\n",
        "# np.unique(trainy)\r\n",
        "trainy.T.shape\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jriaq5o4GmyT"
      },
      "source": [
        "def softmax(arr):\r\n",
        "    \r\n",
        "    l = []\r\n",
        "\r\n",
        "    flag = False\r\n",
        "\r\n",
        "    cnt = 0\r\n",
        "  \r\n",
        "    y = []\r\n",
        "    for i in range(arr.shape[0]) : \r\n",
        "      \r\n",
        "      if(arr[i][0]<=600):\r\n",
        "         l.append(math.exp(arr[i][0]))\r\n",
        "      else:\r\n",
        "        flag = True\r\n",
        "        cnt = cnt + 1\r\n",
        "        y.append(i)\r\n",
        "#      print(i)\r\n",
        "\r\n",
        "    if(flag):\r\n",
        "       \r\n",
        "      # print('array length in softmax',len(arr))\r\n",
        "       temp = np.zeros(len(arr))\r\n",
        "    \r\n",
        "       for it in y:\r\n",
        "       #   print(type(it),type(temp))\r\n",
        "          temp[it] = 1.0/cnt;\r\n",
        "\r\n",
        "       temp = temp.reshape(temp.shape[0],1) \r\n",
        "          \r\n",
        "       return temp   \r\n",
        "         \r\n",
        "    if(np.sum(l) != 0):\r\n",
        "\r\n",
        "      l = np.array(l)\r\n",
        "      l = (l)/np.sum(l)\r\n",
        "\r\n",
        "      l = l.reshape((l.shape[0],1))\r\n",
        "\r\n",
        "      return l\r\n",
        "\r\n",
        "    else:\r\n",
        "       return (1.0/len(arr))*(np.ones(shape=(len(arr),1),dtype=float))\r\n",
        "\r\n",
        "\r\n",
        "def feed_forward(X,activation_function,weights,biases,output_function=softmax):\r\n",
        "    \r\n",
        "    X = X.copy()\r\n",
        "    X = X.flatten()\r\n",
        "    num_hidden_layers = len(biases)\r\n",
        "    # X is the input (x1,x2,...,xn)\r\n",
        "    # activation_function is just the activation function s.t h_i = activation_function(a_i)\r\n",
        "    # weights is a list of m*n where n is number of neurons in previous layer and m is number of neurons in current layer assume 1 based indexing \r\n",
        "    # assuming output function O is softmax function\r\n",
        "    # biases is list of  [b1,b2,..,bk] for each layer\r\n",
        "\r\n",
        "    X = X.reshape((X.shape[0],1))\r\n",
        "    a = []\r\n",
        "    h = []\r\n",
        "    \r\n",
        "    for i in range(num_hidden_layers):\r\n",
        "       if(i == 0):\r\n",
        "         a.append(np.array(0))\r\n",
        "         h.append(X)\r\n",
        "       else:\r\n",
        "        # a_i = W_ih_(i-1) + b_i\r\n",
        "        #print(weights[i].shape,h[i-1].shape,biases[i].shape)\r\n",
        "        # print(weights[i],h[i-1])\r\n",
        "        a.append(np.matmul(weights[i],h[i-1])+biases[i])\r\n",
        "        #print(a[len(a)-1].shape)\r\n",
        "        h.append(activation_function(a[len(a)-1]))\r\n",
        "    # print(\"a is \")\r\n",
        "    # for it in a:\r\n",
        "    #    print(it.flatten())\r\n",
        "    # print(\"####\")\r\n",
        "    # print(\"h is \")\r\n",
        "    # for i in range(4):\r\n",
        "    #    if(i != 0):\r\n",
        "    #       print(h[i].flatten())    \r\n",
        "    return [a,h,output_function(a[len(a)-1])]\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CajlrtTCDS9"
      },
      "source": [
        "def upd_lst(lst1,lst2,operation,eeta): #Performs x 'operation' eeta*y for each element x in lst1 and y in lst2. 'operation' - addition/subtraction\r\n",
        "   l = []\r\n",
        "   if(len(lst1)!=len(lst2)):\r\n",
        "    print(\"Lists are not of equal length\")\r\n",
        " #  print(len(lst1),len(lst2))\r\n",
        "   for (x,y) in zip(lst1,lst2):\r\n",
        "   #  if(type(x) != int and type(y) !=int):\r\n",
        "   #     print('....',x.shape,y.shape)\r\n",
        "     if(operation == '+'):\r\n",
        "       l.append(x+(eeta*y))\r\n",
        "     elif(operation == '-'):\r\n",
        "       l.append(x-(eeta*y))\r\n",
        "     elif(operation == '*'):\r\n",
        "       l.append(x*y*eeta*eeta)\r\n",
        "     elif(operation == 'sqrt'):\r\n",
        "      # print(x,y,eeta)\r\n",
        "       l.append(x/np.sqrt(y+eeta))\r\n",
        "     elif(operation=='mult'):\r\n",
        "      l.append(x*eeta)\r\n",
        "\r\n",
        "   return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbZQU9bCsCJi"
      },
      "source": [
        "def calculate_gradients(y_true,y_out,h,a,weights,act_fn_derivative,weight_decay,biases):\r\n",
        "    \r\n",
        "    #delta(A_l) = -(e_ytrue - y_out)\r\n",
        "    #delta(h_i) = (W_i+1).Tdelta(A_i+1)  (for i = L-1 to 1)\r\n",
        "    #delta(A_i) = delta(h_i)*(activation_function'(A_i)) for(i = L-1 to 1)\r\n",
        "    #delta(W_k) = delta(A_k)(h_(k-1).T) for(i = L to i = 1)\r\n",
        "    #delta(b_k) = delta(A_k)\r\n",
        "\r\n",
        "    e = np.zeros(len(y_out))\r\n",
        "    e = e.reshape(e.shape[0],1)\r\n",
        "    e[y_true] = 1\r\n",
        "    A = y_out-e\r\n",
        "    l = len(h)\r\n",
        "    delta_W = []\r\n",
        "    delta_B = []\r\n",
        "    \r\n",
        "    delta_W.append(np.matmul(A,h[l-2].T))\r\n",
        "#    print(delta_W[0].shape,weights[l-1].shape)\r\n",
        "    delta_B.append(A)\r\n",
        "    l = l-2\r\n",
        "\r\n",
        "    while(l >= 1):\r\n",
        "       H = np.matmul(weights[l+1].T,A)\r\n",
        "       A = H*(act_fn_derivative(a[l]))     # assuming a is 1 based index \r\n",
        "       delta_W.append(np.matmul(A,h[l-1].T))\r\n",
        " #      print(delta_W[len(delta_W)-1].shape,weights[l].shape)\r\n",
        "       delta_B.append(A)\r\n",
        "       l = l-1\r\n",
        "    \r\n",
        "    delta_W.append(0)\r\n",
        "    delta_B.append(0)\r\n",
        "    delta_W.reverse()\r\n",
        "    delta_B.reverse()\r\n",
        "    delta_W=upd_lst(delta_W,weights,'+',weight_decay)\r\n",
        "    delta_B=upd_lst(delta_B,biases,'+',weight_decay)\r\n",
        "    return [delta_W,delta_B]\r\n",
        "\r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-FV-Ijsclo"
      },
      "source": [
        "def sgd(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "    \r\n",
        "    for i in range(num_epochs): \r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "            cnt = cnt + 1\r\n",
        "            r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "            a = r[0]\r\n",
        "            h = r[1]\r\n",
        "            yout = r[2]\r\n",
        "            # print(yout)\r\n",
        "            # if(cnt == 2):\r\n",
        "            #    break\r\n",
        "            gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "            cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "            cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "\r\n",
        "            \r\n",
        "            if(cnt == batch_size): \r\n",
        "                # cum_W = upd_lst(cum_W,weights,'-',(weight_decay)*batch_size)\r\n",
        "                # cum_B = upd_lst(cum_B,biases,'-',(weight_decay)*batch_size)\r\n",
        "                weights = upd_lst(weights,cum_W,'-',(learning_rate*(1.0))/batch_size)\r\n",
        "                biases = upd_lst(biases,cum_B,'-',(learning_rate*(1.0))/batch_size)\r\n",
        "                cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "                cnt = 0\r\n",
        "          print(i)\r\n",
        "    return [weights,biases]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkwA09bFEAzb"
      },
      "source": [
        "def adam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "\r\n",
        "    eps = 1e-8\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    for i in range(num_epochs): \r\n",
        "\r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "          m_W = cum_W\r\n",
        "          m_B = cum_B\r\n",
        "\r\n",
        "          v_W = cum_W\r\n",
        "          v_B = cum_B\r\n",
        "\r\n",
        "          a_cnt  = 0\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "             cnt = cnt + 1\r\n",
        "             r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "             cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "             cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "\r\n",
        "             if(cnt == batch_size): \r\n",
        "                 \r\n",
        "                #  cum_W = upd_lst(cum_W,weights,'-',(weight_decay)*batch_size)\r\n",
        "                #  cum_B = upd_lst(cum_B,biases,'-',(weight_decay)*batch_size)\r\n",
        "\r\n",
        "                 m_W = upd_lst(m_W,m_W,'+',(beta1-1))   #m_W = beta1*m_W\r\n",
        "                 m_W = upd_lst(m_W,cum_W,'+',(1-beta1)/batch_size)   #m_W += (1-beta1)*db\r\n",
        "\r\n",
        "                 m_B = upd_lst(m_B,m_B,'+',(beta1-1))   #m_B = beta1*m_B\r\n",
        "                 m_B = upd_lst(m_B,cum_B,'+',(1-beta1)/batch_size)   #m_B += (1-beta1)*db\r\n",
        "                 \r\n",
        "\r\n",
        "                 d_W_sqr = upd_lst(cum_W,cum_W,'*',1.0/batch_size)\r\n",
        "                 d_B_sqr = upd_lst(cum_B,cum_B,'*',1.0/batch_size)\r\n",
        "\r\n",
        "                 v_W = upd_lst(v_W,v_W,'+',(beta2-1))  #v_W = beta2*v_W\r\n",
        "                 v_W = upd_lst(v_W,d_W_sqr,'+',(1-beta2))  #v_W += (1-beta2)*dwsqr\r\n",
        "\r\n",
        "                 v_B = upd_lst(v_B,v_B,'+',(beta2-1))  #v_B = beta2*v_B\r\n",
        "                 v_B = upd_lst(v_B,d_B_sqr,'+',(1-beta2))  #v_B += (1-beta2)*dbsqr\r\n",
        "                 \r\n",
        "                 t = math.pow(beta1,j+1)/(1-math.pow(beta1,a_cnt+1))\r\n",
        "                 m_W_hat = upd_lst(m_W,m_W,'+',t)\r\n",
        "                 m_B_hat = upd_lst(m_B,m_B,'+',t)\r\n",
        "\r\n",
        "                 t = math.pow(beta2,j+1)/(1-math.pow(beta2,a_cnt+1))\r\n",
        "                # print(beta2,math.pow(beta2,a_cnt+1))\r\n",
        "                 v_W_hat = upd_lst(v_W,v_W,'+',t)\r\n",
        "                 v_B_hat = upd_lst(v_B,v_B,'+',t)\r\n",
        "\r\n",
        "                 \r\n",
        "                 m_W_hat = upd_lst(m_W_hat,v_W_hat,'sqrt',eps) #m_W_hat = m_W_hat/(sqrt(v_W_hat+eps))\r\n",
        "                 weights = upd_lst(weights,m_W_hat,'-',learning_rate)   #weights = weights - (learning rate)*m_W_hat/sqrt(v_W_hat+eps)\r\n",
        "\r\n",
        "                 m_B_hat = upd_lst(m_B_hat,v_B_hat,'sqrt',eps) #m_B_hat = m_B_hat/sqrt(V_B_hat+eps)\r\n",
        "                 biases = upd_lst(biases,m_B_hat,'-',learning_rate) #biases = biases - (learning rate)*m_B_hat/sqrt(v_B_hat+eps)\r\n",
        "\r\n",
        "                 cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                 cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "                 cnt = 0\r\n",
        "                 a_cnt = a_cnt + 1\r\n",
        "\r\n",
        "          print(i)\r\n",
        "\r\n",
        "    return [weights,biases]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjpR2fftjnQe"
      },
      "source": [
        "def nadam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "    \r\n",
        "    eps = 1e-8\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    for i in range(num_epochs): \r\n",
        "\r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "          m_W = cum_W\r\n",
        "          m_B = cum_B\r\n",
        "\r\n",
        "          v_W = cum_W\r\n",
        "          v_B = cum_B\r\n",
        "\r\n",
        "          cum_W_m = cum_W\r\n",
        "          cum_B_m = cum_B\r\n",
        "\r\n",
        "          a_cnt  = 0\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "             cnt = cnt + 1\r\n",
        "             r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "             cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "             cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "             \r\n",
        "             weights_m = upd_lst(weights,m_W,'-',beta1)\r\n",
        "             biases_m = upd_lst(weights,m_B,'-',beta1)\r\n",
        "             \r\n",
        "             r = feed_forward(X[j],activation_function,weights_m,biases_m)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights_m,act_fn_derivative,weight_decay,biases)\r\n",
        "\r\n",
        "             cum_W_m = upd_lst(cum_W_m,gradients[0],'+',1)\r\n",
        "             cum_B_m = upd_lst(cum_B_m,gradients[1],'+',1)\r\n",
        "             \r\n",
        "\r\n",
        "             if(cnt == batch_size):\r\n",
        "                 \r\n",
        "                #  cum_W_m = upd_lst(cum_W_m,weights,'-',(weight_decay)*batch_size)\r\n",
        "                #  cum_B_m = upd_lst(cum_B_m,biases,'-',(weight_decay)*batch_size)\r\n",
        "\r\n",
        "                #  cum_W = upd_lst(cum_W,weights,'-',(weight_decay)*batch_size)\r\n",
        "                #  cum_B = upd_lst(cum_B,biases,'-',(weight_decay)*batch_size)\r\n",
        "                 \r\n",
        "                 m_W = upd_lst(m_W,m_W,'+',(beta1-1))   #m_W = beta1*m_W\r\n",
        "                 m_W = upd_lst(m_W,cum_W_m,'+',(1-beta1)/batch_size)   #m_W += (1-beta1)*db\r\n",
        "\r\n",
        "                 m_B = upd_lst(m_B,m_B,'+',(beta1-1))   #m_B = beta1*m_B\r\n",
        "                 m_B = upd_lst(m_B,cum_B_m,'+',(1-beta1)/batch_size)   #m_B += (1-beta1)*db\r\n",
        "                 \r\n",
        "\r\n",
        "                 d_W_sqr = upd_lst(cum_W,cum_W,'*',1.0/batch_size)\r\n",
        "                 d_B_sqr = upd_lst(cum_B,cum_B,'*',1.0/batch_size)\r\n",
        "\r\n",
        "                 v_W = upd_lst(v_W,v_W,'+',(beta2-1))  #v_W = beta2*v_W\r\n",
        "                 v_W = upd_lst(v_W,d_W_sqr,'+',(1-beta2))  #v_W += (1-beta2)*dwsqr\r\n",
        "\r\n",
        "                 v_B = upd_lst(v_B,v_B,'+',(beta2-1))  #v_B = beta2*v_B\r\n",
        "                 v_B = upd_lst(v_B,d_B_sqr,'+',(1-beta2))  #v_B += (1-beta2)*dbsqr\r\n",
        "                 \r\n",
        "                 t = math.pow(beta1,j+1)/(1-math.pow(beta1,a_cnt+1))\r\n",
        "                 m_W_hat = upd_lst(m_W,m_W,'+',t)\r\n",
        "                 m_B_hat = upd_lst(m_B,m_B,'+',t)\r\n",
        "\r\n",
        "                 t = math.pow(beta2,j+1)/(1-math.pow(beta2,a_cnt+1))\r\n",
        "                # print(beta2,math.pow(beta2,a_cnt+1))\r\n",
        "                 v_W_hat = upd_lst(v_W,v_W,'+',t)\r\n",
        "                 v_B_hat = upd_lst(v_B,v_B,'+',t)\r\n",
        "\r\n",
        "                 \r\n",
        "                 m_W_hat = upd_lst(m_W_hat,v_W_hat,'sqrt',eps) #m_W_hat = m_W_hat/(sqrt(v_W_hat+eps))\r\n",
        "                 weights = upd_lst(weights,m_W_hat,'-',learning_rate)   #weights = weights - (learning rate)*m_W_hat/sqrt(v_W_hat+eps)\r\n",
        "\r\n",
        "                 m_B_hat = upd_lst(m_B_hat,v_B_hat,'sqrt',eps) #m_B_hat = m_B_hat/sqrt(V_B_hat+eps)\r\n",
        "                 biases = upd_lst(biases,m_B_hat,'-',learning_rate) #biases = biases - (learning rate)*m_B_hat/sqrt(v_B_hat+eps)\r\n",
        "\r\n",
        "                 cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                 cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "                 cum_W_m = cum_W\r\n",
        "                 cum_B_m = cum_B\r\n",
        "\r\n",
        "                 cnt = 0\r\n",
        "                 a_cnt = a_cnt + 1\r\n",
        "\r\n",
        "          print(i)\r\n",
        "\r\n",
        "    return [weights,biases]\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwreguEzpKGE"
      },
      "source": [
        "def momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(weight)\r\n",
        "  #weigt{t}=weight{t-1}-update{t}\r\n",
        "  gamma=0.9 # The typical value of gamma used is 0.9\r\n",
        "  for lmao in range(num_epochs):\r\n",
        "    #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "    weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "    #Initialisation of update_weights\r\n",
        "    update_weights=weights_grad_tot\r\n",
        "    update_biases=biases_grad_tot\r\n",
        "\r\n",
        "    print(lmao)\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "    dum=0\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "      #print(dum)\r\n",
        "      #dum=dum+1\r\n",
        "      y_pred=feed_forward(x,activation_function,weights,biases)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        # weights_grad_tot = upd_lst(weights_grad_tot,weights,'-',(weight_decay)*batch_size)\r\n",
        "        # biases_grad_tot = upd_lst(biases_grad_tot,biases,'-',(weight_decay)*batch_size)\r\n",
        "        update_weights = upd_lst(update_weights,update_weights,'+',gamma-1)\r\n",
        "        update_biases = upd_lst(update_biases,update_biases,'+',gamma-1)\r\n",
        "        update_weights=upd_lst(update_weights,weights_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        update_biases=upd_lst(update_biases,biases_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        # update_weights = update_weights + gamma*update_weights + learning_rate * weights_grad_tot\r\n",
        "        # update_biases = update_biases + gamma*update_weight + learning_rate * biases_grad_tot\r\n",
        "        weights=upd_lst(weights,update_weights,'-',1.0)\r\n",
        "        biases=upd_lst(biases,update_biases,'-',1.0)\r\n",
        "        num_data_point_done=0\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]\r\n",
        "  \r\n",
        "def nesterov(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "\r\n",
        "  gamma=0.9 # The typical value of gamma used is 0.9\r\n",
        "  #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "  weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "  biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "  #Initialisation of update_weights\r\n",
        "  update_weights=weights_grad_tot\r\n",
        "  update_biases=biases_grad_tot  \r\n",
        "  for lmao in range(num_epochs):\r\n",
        "    print(lmao)\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "    w_ahead=weights # Initial value of w_ahead is weights\r\n",
        "    b_ahead=biases\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "      y_pred=feed_forward(x,activation_function,w_ahead,b_ahead)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,w_ahead,act_fn_derivative,weight_decay,b_ahead)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)   \r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        # weights_grad_tot = upd_lst(weights_grad_tot,weights,'-',(weight_decay)*batch_size)\r\n",
        "        # biases_grad_tot = upd_lst(biases_grad_tot,biases,'-',(weight_decay)*batch_size)\r\n",
        "        w_ahead=upd_lst(weights,update_weights,'-',gamma)\r\n",
        "        b_ahead=upd_lst(biases,update_biases,'-',gamma)\r\n",
        "        update_weights = upd_lst(update_weights,update_weights,'+',gamma-1)\r\n",
        "        update_biases = upd_lst(update_biases,update_biases,'+',gamma-1)\r\n",
        "        update_weights=upd_lst(update_weights,weights_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        update_biases=upd_lst(update_biases,biases_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        # update_weights = update_weights + gamma*update_weights + learning_rate * weights_grad_tot\r\n",
        "        # update_biases = update_biases + gamma*update_weight + learning_rate * biases_grad_tot\r\n",
        "        weights=upd_lst(weights,update_weights,'-',1.0)\r\n",
        "        biases=upd_lst(biases,update_biases,'-',1.0)\r\n",
        "        num_data_point_done=0\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]\r\n",
        "\r\n",
        "\r\n",
        "  \r\n",
        "# def adagrad(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "#   #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "#   #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "#   # w_ahead= weights\r\n",
        "#   # b_ahead=biases\r\n",
        "#   for i in range(num_epochs):\r\n",
        "#     # num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "#     Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "#     weights_grad_tot=[]\r\n",
        "#     for w in weights:\r\n",
        "#       weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "#     biases_grad_tot=[]\r\n",
        "#     for b in biases:\r\n",
        "#       biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "#     num_data_point_done=0\r\n",
        "#     v_weight = 0 #Intitial value of 0\r\n",
        "#     v_bias=0\r\n",
        "#     epsilon = 0 #Figure out what is epsilon\r\n",
        "#     for x,y,y_pred in zip(X,Y,Y_pred):    \r\n",
        "#       [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,weights,act_fn_derivative)\r\n",
        "#       weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "#       biases_grad_tot = biases_grad_tot + biases_grad\r\n",
        "#       num_data_point_done=num_data_point_done+1\r\n",
        "#       if(num_data_point_done==batch_size):\r\n",
        "#         v_weight = v_weight + (weights_grad_total)**2\r\n",
        "#         v_bias = v_bias + (biases_grad_total)**2\r\n",
        "#         v_weight_dum = learning_rate/np.sqrt(v_weight+epsilon)\r\n",
        "#         v_bias_dum = learning_rate/np.sqrt(v_bias+epsilon)\r\n",
        "#         weights=weights- v_weight_dum * weights_grad_tot\r\n",
        "#         biases=biases - v_bias_dum * bias_grad_tot\r\n",
        "#         num_data_point_done=0\r\n",
        "    \r\n",
        "def rmsprop(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  beta=0.9 # The typical value of beta used is 0.9\r\n",
        "  epsilon = 1e-8  # Typical value of epsilon used\r\n",
        "  for lmao in range(num_epochs):\r\n",
        "    #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "    weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "    v_weight = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    b_weight = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]   \r\n",
        "    print(lmao)\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "    \r\n",
        "      y_pred=feed_forward(x,activation_function,weights,biases)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):   \r\n",
        "        # weights_grad_tot = upd_lst(weights_grad_tot,weights,'-',(weight_decay)*batch_size)\r\n",
        "        # biases_grad_tot = upd_lst(biases_grad_tot,biases,'-',(weight_decay)*batch_size)\r\n",
        "        v_weight = upd_lst(v_weight,v_weight,'+',(beta-1))\r\n",
        "        dum1=upd_lst(weights_grad_tot,weights_grad_tot,'*',1.0/batch_size)\r\n",
        "        v_weight= upd_lst(v_weight,dum1,'+',(1-beta))\r\n",
        "        b_weight = upd_lst(b_weight,b_weight,'+',(beta-1))\r\n",
        "        dum2=upd_lst(biases_grad_tot,biases_grad_tot,'*',1.0/batch_size)\r\n",
        "        b_weight= upd_lst(b_weight,dum2,'+',(1-beta))\r\n",
        "        weights_grad_tot=upd_lst(weights_grad_tot,weights_grad_tot,'mult',1.0/batch_size)\r\n",
        "\r\n",
        "        v_weight_dum=upd_lst(weights_grad_tot,v_weight,'sqrt',epsilon)\r\n",
        "        weights=upd_lst(weights, v_weight_dum,'-',learning_rate)\r\n",
        "        biases_grad_tot=upd_lst(biases_grad_tot,biases_grad_tot,'mult',1.0/batch_size)\r\n",
        "\r\n",
        "        b_weight_dum=upd_lst(biases_grad_tot,b_weight,'sqrt',epsilon)\r\n",
        "        biases=upd_lst(biases, b_weight_dum,'-',learning_rate)       \r\n",
        "        num_data_point_done=0\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuklB5S78jQy"
      },
      "source": [
        "def backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay,validateX,validatey):\r\n",
        "    \r\n",
        "    \r\n",
        "    #do the weights Initialisation here based on weight_initialisation,num_hidden_layers,sz_each_hidden_layer\r\n",
        "\r\n",
        "    #bias is 0 initialised make that change\r\n",
        "    \r\n",
        "    weights = [0]  #dummy value\r\n",
        "    biases =  [0]  #dummy value \r\n",
        "    \r\n",
        "    if(activation=='sigmoid'):\r\n",
        "      activation_function=logistic\r\n",
        "      act_fn_derivative=log_grad\r\n",
        "    elif(activation=='tanh'):\r\n",
        "      activation_function=tanh\r\n",
        "      act_fn_derivative=tanh_grad\r\n",
        "    elif(activation=='ReLU'):\r\n",
        "      activation_function=relu\r\n",
        "      act_fn_derivative=relu_grad\r\n",
        "\r\n",
        "\r\n",
        "    if (weight_initialisation == 'random'):\r\n",
        "        \r\n",
        "        num_classes = len(np.unique(Y))\r\n",
        "        i = len(X[0].flatten())\r\n",
        "        a = -1\r\n",
        "        b = 1\r\n",
        "\r\n",
        "        for j in range(num_hidden_layers-1):\r\n",
        "            weights.append((b-a)*np.random.rand(sz_hidden_layer,i)+a)\r\n",
        "            biases.append(np.zeros((sz_hidden_layer,1)))\r\n",
        "            i = sz_hidden_layer\r\n",
        "\r\n",
        "        weights.append((b-a)*np.random.rand(num_classes,i)+a)\r\n",
        "        biases.append(np.zeros((num_classes,1)))\r\n",
        "\r\n",
        "    else:\r\n",
        "\r\n",
        "        #Xavier Implementation\r\n",
        "        num_classes = len(np.unique(Y))\r\n",
        "        i = len(X[0].flatten())\r\n",
        "\r\n",
        "        for j in range(num_hidden_layers-1):\r\n",
        "            t = math.sqrt(6)/math.sqrt(i + sz_hidden_layer)\r\n",
        "            weights.append(np.random.uniform(-t,t,(sz_hidden_layer,i)))\r\n",
        "            biases.append(np.zeros((sz_hidden_layer,1)))\r\n",
        "            i = sz_hidden_layer\r\n",
        "        \r\n",
        "        t = math.sqrt(6)/math.sqrt(i + num_classes)\r\n",
        "        weights.append(np.random.uniform(-t,t,(num_classes,i)))\r\n",
        "        biases.append(np.zeros((num_classes,1)))\r\n",
        "\r\n",
        "    fun = 1\r\n",
        "    if (optimizer == 'sgd'):\r\n",
        "        fun = sgd\r\n",
        "    elif (optimizer == 'momentum'):\r\n",
        "        fun = momentum\r\n",
        "    elif (optimizer == 'nesterov'):\r\n",
        "        fun = nesterov\r\n",
        "    elif (optimizer == 'rmsprop'):\r\n",
        "        fun = rmsprop\r\n",
        "    elif (optimizer == 'adam'):\r\n",
        "        fun = adam\r\n",
        "    elif (optimizer == 'nadam'):\r\n",
        "        fun = nadam\r\n",
        "\r\n",
        "    for i in range(num_epochs):\r\n",
        "\r\n",
        "      #backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay)\r\n",
        "      [weights,biases] = fun(X,Y,1,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative) #backpropagation(trainX,trainy,1,config.num_hidden_layer,config.size_layer,config.learning_rate,config.optimizer,config.batch_size,config.weight_intialisation,config.activation,config.weight_decay)\r\n",
        "      # print('val_accuracy'+str(accuracy(validateX,validatey,weights,biases)))\r\n",
        "      # print('val_loss'+str(cross_entropy_loss(validateX,validatey,weights,biases)))\r\n",
        "      # print('accuracy'+str(accuracy(X,Y,weights,biases)))\r\n",
        "      # print('loss'+str(cross_entropy_loss(X,Y,weights,biases)))\r\n",
        "      wandb.log({'epoch':i})\r\n",
        "      wandb.log({'val_accuracy':accuracy(validateX,validatey,weights,biases)})\r\n",
        "      wandb.log({'val_loss':cross_entropy_loss(validateX,validatey,weights,biases)})\r\n",
        "      wandb.log({'accuracy':accuracy(X,Y,weights,biases)})\r\n",
        "      wandb.log({'loss':cross_entropy_loss(X,Y,weights,biases)})\r\n",
        "      wandb.log({'epoch':i})\r\n",
        "\r\n",
        "\r\n",
        "    # if(optimizer == 'sgd'):\r\n",
        "    #    return sgd(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    # if(optimizer == 'momentum'): \r\n",
        "    #    return momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    # if(optimizer == 'nesterov'): \r\n",
        "    #   return nesterov(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    # if(optimizer == 'rmsprop'):\r\n",
        "    #   return rmsprop(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    # if(optimizer == 'adam'):\r\n",
        "    #   return adam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    # if(optimizer == 'nadam'):\r\n",
        "    #   return nadam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKnNVKLFKZuW"
      },
      "source": [
        "# import warnings\r\n",
        "# warnings.simplefilter('error', RuntimeWarning) # To help debug where the warnings are\r\n",
        "#backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay):\r\n",
        "#[weights,biases] = backpropagation(trainX,trainy,5,3,32,1e-3,'sgd',16,'random','sigmoid',0) #logistic,log_grad)\r\n",
        "# # [weights_momentum,biases_momentum] = backpropagation(trainX/255,trainy,5,3,32,0,1e-3,'momentum',16,'random',logistic,log_grad)\r\n",
        "#def backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay,validateX,validatey):\r\n",
        "# backpropagation(trainX,trainy,5,3,32,1e-3,'nesterov',64,'random','sigmoid',0.0005,validateX,validatey)\r\n",
        "# #[weights_rmsprop,biases_rmsprop] = backpropagation(trainX/255,trainy,5,3,32,0,1e-3,'rmsprop',16,'random',logistic,log_grad)\r\n",
        "# #t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# # t_cross_momentum = cross_entropy_loss(testX, testy,weights_momentum,biases_momentum)\r\n",
        "# #t_cross_nestrov = cross_entropy_loss(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# # t_cross_rmsprop = cross_entropy_loss(testX, testy,weights_rmsprop,biases_rmsprop)\r\n",
        "\r\n",
        "# #print(t)\r\n",
        "# # print(t_cross_momentum)\r\n",
        "# #print(t_cross_nestrov)\r\n",
        "# print(t_cross_rmsprop)\r\n",
        "\r\n",
        "# #t = accuracy(testX, testy,weights,biases)\r\n",
        "# # t_momentum=accuracy(testX, testy,weights_momentum,biases_momentum)\r\n",
        "# #t_nestrov=accuracy(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# t_rmsprop=accuracy(testX, testy,weights_rmsprop,biases_rmsprop)\r\n",
        "# #print(t)\r\n",
        "# # # print(t_momentum)\r\n",
        "# # print(t_rmsprop)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XQxbe9UHc9Q"
      },
      "source": [
        "# [a,b] = [1,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtSsP1XXK3cZ"
      },
      "source": [
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,0,1e-3,'sgd',16,'random',tanh,tanh_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tECJIeAMK4Gn"
      },
      "source": [
        "# t = cross_entropy_loss(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxjxvnsdK415"
      },
      "source": [
        "# t = accuracy(testX, testy,weights_nestrov,biases_nestrov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKJcl0Grh-MH"
      },
      "source": [
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJYP8kMnjTxt"
      },
      "source": [
        "# np.random.uniform(-5,5,(2,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVMV1AF0kZYJ"
      },
      "source": [
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,0,1e-3,'sgd',16,'xavier',logistic,log_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuJdoaH_nrhQ"
      },
      "source": [
        "# t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ179Ut4n4aS"
      },
      "source": [
        "# t = accuracy(testX, testy,weights,biases)\r\n",
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HgpqKs9n59c"
      },
      "source": [
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,0,1e-3,'sgd',16,'xavier',tanh,tanh_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArcNEpNxoCYZ"
      },
      "source": [
        "# t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuDghqYzoEQE"
      },
      "source": [
        "# t = accuracy(testX, testy,weights,biases)\r\n",
        "# print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFuD9Bu7ojO5"
      },
      "source": [
        "num_epochs = [5,10,15]\r\n",
        "\r\n",
        "weight_decay = [0,0.0005,0.5]\r\n",
        "learning_rate = [1e-3,1e-4]\r\n",
        "optimiser = ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\r\n",
        "batch_size =[16, 32, 64]\r\n",
        "\r\n",
        "activation= ['sigmoid', 'tanh', 'ReLU']\r\n",
        "\r\n",
        "weight_initialisation = [ 'random', 'Xavier']\r\n",
        "num_hidden_layer = [3,4,5]\r\n",
        "size_layer = [32,64,128]\r\n",
        "sweep_config = {\r\n",
        "    'name': 'max_accuracy_1.2',\r\n",
        "    'method' : 'random',\r\n",
        "    'metric': {        \r\n",
        "         'goal': 'maximize',\r\n",
        "         'name' : 'val_accuracy'\r\n",
        "    },\r\n",
        "    'parameters': {\r\n",
        "        'epochs':{\r\n",
        "            'values': [10]\r\n",
        "        },\r\n",
        "        'batch_size':{\r\n",
        "            'values': [64]\r\n",
        "        },\r\n",
        "        'weight_decay':{\r\n",
        "            'values': [0.0005]\r\n",
        "        },\r\n",
        "        'learning_rate':{\r\n",
        "            'values': [0.001]\r\n",
        "        },\r\n",
        "        'optimizer':{\r\n",
        "            'values': ['adam']\r\n",
        "        },\r\n",
        "        'activation':{\r\n",
        "            'values': ['sigmoid']\r\n",
        "        },\r\n",
        "        'weight_intialisation':{\r\n",
        "            'values': ['random']\r\n",
        "        },\r\n",
        "        'num_hidden_layer': {\r\n",
        "            'values': [5]\r\n",
        "        },\r\n",
        "        'size_layer':{\r\n",
        "            'values': [128]\r\n",
        "        }\r\n",
        "\r\n",
        "    }\r\n",
        "}\r\n",
        "    # wandb.log({{'val_loss': })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5Zu-AD9YRF"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1G4h-4HOvpu"
      },
      "source": [
        "def train():\r\n",
        "  config_default = {\r\n",
        "        'epochs': 10,\r\n",
        "        'batch_size': 64,\r\n",
        "        'weight_decay':0.0005, \r\n",
        "        'learning_rate':0.001,\r\n",
        "        'optimizer': 'adam',\r\n",
        "        'activation': 'sigmoid', \r\n",
        "        'weight_intialisation':'random', \r\n",
        "        'num_hidden_layer': 5,\r\n",
        "        'size_layer': 128\r\n",
        "  }\r\n",
        "  print(\"wand about to be initialised\")\r\n",
        "  #config=wandb.config\r\n",
        "  #name = 'epochs_'+str(config.epochs)+'_btch_sz_'+str(config.batch_size)+'_wght_dcy_'+str(config.weight_decay)+'_optm_'+config.optimizer+'_lrng_rat_'+config.learning_rate+'_actvn_'+config.activation+'_numhdnlyr_'+str(config.num_hidden_layer)+' sz_lyr_'+str(config.size_layer)\r\n",
        "  wandb.init(config=config_default,project='cs6910-assignment1')\r\n",
        "  config = wandb.config\r\n",
        "  a = trainX\r\n",
        "  b = trainy\r\n",
        "  \r\n",
        "  backpropagation(trainX,trainy,config.epochs,config.num_hidden_layer,config.size_layer,config.learning_rate,config.optimizer,config.batch_size,config.weight_intialisation,config.activation,config.weight_decay,validateX,validatey)\r\n",
        "    \r\n",
        "\r\n",
        "  \r\n",
        "      \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FtTIclCSSCE"
      },
      "source": [
        "#agent=wandb.init(entity=\"nvvishnu\",project=\"cs6910-assignment1\")\r\n",
        "# sweep_config['parameters']['epochs']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJRqfvgM-O-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf787a5d-9add-4fdd-e036-4473d3d90376"
      },
      "source": [
        "# # import warnings\r\n",
        "# # warnings.simplefilter('error', RuntimeWarning) # To help debug where the warnings are\r\n",
        "sweep_id = wandb.sweep(sweep_config,entity=\"nvvishnu\",project=\"cs6910-assignment1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vb52k0il\n",
            "Sweep URL: https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icMMODn3Sh26"
      },
      "source": [
        "# sweep_id = 'c80h26c5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq70j4ZKT3TC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbf4ce3667d243689f2a3149a146fe76",
            "47c9f9b4911a4d60a22a6a6f9d4f2422",
            "dd1b93d89a71420c95886611f03e8ac4",
            "de0e9bd86b124fd9a8e3934a0a52d440",
            "573bd6efba9c4f0faeabc09149be366c",
            "9d59b9ce1299431da8640288100bed8a",
            "f78b13f25165442d91354605acf682ba",
            "d292200ee4d64960b94fe165bbb6aa98"
          ]
        },
        "outputId": "4d41fcfd-74d7-464e-8e05-31dd6354e0e3"
      },
      "source": [
        "import warnings\r\n",
        "warnings.simplefilter('error', RuntimeWarning) # To help debug where the warnings are\r\n",
        "wandb.agent(sweep_id,project=\"cs6910-assignment1\",function=train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ot4gebxl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_intialisation: random\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wand about to be initialised\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">snowy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_100131-ot4gebxl</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 871<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf4ce3667d243689f2a3149a146fe76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210313_100131-ot4gebxl/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210313_100131-ot4gebxl/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>9</td></tr><tr><td>_runtime</td><td>7360</td></tr><tr><td>_timestamp</td><td>1615637051</td></tr><tr><td>_step</td><td>59</td></tr><tr><td>val_accuracy</td><td>0.8635</td></tr><tr><td>val_loss</td><td>0.38837</td></tr><tr><td>accuracy</td><td>0.87815</td></tr><tr><td>loss</td><td>0.34936</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▆▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr><tr><td>accuracy</td><td>▁▃▃▄▅▆▆▇██</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">snowy-sweep-1</strong>: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5trei2ti with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_intialisation: random\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wand about to be initialised\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">jolly-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/5trei2ti\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/5trei2ti</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_120416-5trei2ti</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 926<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pU4oxPDhOMt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}