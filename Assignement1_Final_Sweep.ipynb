{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Assignement1_Final_Sweep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbf4ce3667d243689f2a3149a146fe76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47c9f9b4911a4d60a22a6a6f9d4f2422",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dd1b93d89a71420c95886611f03e8ac4",
              "IPY_MODEL_de0e9bd86b124fd9a8e3934a0a52d440"
            ]
          }
        },
        "47c9f9b4911a4d60a22a6a6f9d4f2422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd1b93d89a71420c95886611f03e8ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_573bd6efba9c4f0faeabc09149be366c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d59b9ce1299431da8640288100bed8a"
          }
        },
        "de0e9bd86b124fd9a8e3934a0a52d440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f78b13f25165442d91354605acf682ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d292200ee4d64960b94fe165bbb6aa98"
          }
        },
        "573bd6efba9c4f0faeabc09149be366c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d59b9ce1299431da8640288100bed8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f78b13f25165442d91354605acf682ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d292200ee4d64960b94fe165bbb6aa98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvvishnu/CS6910-Assignments/blob/main/Assignement1_Final_Sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2jhbImHGSyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe3b848-b6a3-4c77-8a29-c1da7b65f2f6"
      },
      "source": [
        "#Import the necessary packages\r\n",
        "\r\n",
        "!pip install wandb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import wandb\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot\r\n",
        "import math\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import seaborn as sns #To plot confusion matrix as a heatmap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 33.7MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 24.9MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=bf1391ba76d3b65d3643fa9f1513ea8a2858436a901cfe5a2c5f08c9a8947293\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=b8729baa84dd7da13c486f9d4e47afd5d3cd506030df824bb7a4aea3a2348ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: pathtools, shortuuid, smmap, gitdb, GitPython, docker-pycreds, sentry-sdk, configparser, subprocess32, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1OfVTIUo_g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d0213a-5509-456d-9258-b1584b0465ad"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data() #Loading the testing and training datset of fashion_mnist dataset\r\n",
        "Xtrain = trainX\r\n",
        "Ytrain = trainy\r\n",
        "trainX, validateX, trainy, validatey = train_test_split(trainX,trainy, test_size=0.1, random_state=1) #Split trainining data into training and validation data\r\n",
        "\r\n",
        "#Normalization of training date\r\n",
        "trainX = trainX/255\r\n",
        "testX = testX/255\r\n",
        "validateX = validateX/255"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "O781OEmbGhxi",
        "outputId": "1b603d33-9165-42e3-b788-f9fa145735a0"
      },
      "source": [
        "#Logging into wandb; Intializing wandb with appropriate user name and project\r\n",
        "!wandb login \r\n",
        "wandb.init(project=\"cs6910-assignment1\", entity=\"nvvishnu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnvvishnu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">custard-bun-280</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/1zxdvqad\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/1zxdvqad</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_152642-1zxdvqad</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f5811c9d850>"
            ],
            "text/html": [
              "<h1>Run(1zxdvqad)</h1><iframe src=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/1zxdvqad\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYmwBg_OYBZK"
      },
      "source": [
        "# The ten fashion class labels in fashion_mnist are as follows:\r\n",
        "\r\n",
        "# T-shirt/top\r\n",
        "# Trouser/pants\r\n",
        "# Pullover shirt\r\n",
        "# Dress\r\n",
        "# Coat\r\n",
        "# Sandal\r\n",
        "# Shirt\r\n",
        "# Sneaker\r\n",
        "# Bag\r\n",
        "# Ankle boot\r\n",
        "\r\n",
        "names = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\r\n",
        "images = []\r\n",
        "labels = []\r\n",
        "\r\n",
        "for i in range(Xtrain.shape[0]):\r\n",
        "   if(len(labels) == 10):\r\n",
        "      break\r\n",
        "   if(names[Ytrain[i]] not in labels):\r\n",
        "      images.append(Xtrain[i])\r\n",
        "      labels.append(names[Ytrain[i]])\r\n",
        "   \r\n",
        "wandb.log({\"DataSet Examples\": [wandb.Image(image,caption=label) for image,label in zip(images,labels)]})\r\n",
        "\r\n",
        "#Log sample images of each class to wandb\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MOAEoEsMQUR"
      },
      "source": [
        "#This cell contains the definition of helper functions\r\n",
        "\r\n",
        "#Definition of sigmoid activation function\r\n",
        "def logistic(z):\r\n",
        "  l = []\r\n",
        "\r\n",
        "  for i in z:\r\n",
        "     if (i[0]<-600): #To handle overflows\r\n",
        "       l.append(0)\r\n",
        "     else:\r\n",
        "       l.append(1./(1+math.exp(-i[0])))\r\n",
        "  \r\n",
        "  t = np.array(l)\r\n",
        "\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "\r\n",
        "  return t\r\n",
        "\r\n",
        "#Definition of ReLU activation function\r\n",
        "def relu(z):\r\n",
        "  l=[]\r\n",
        "  for i in z:\r\n",
        "    l.append(max(0,i[0]))\r\n",
        "  t= np.array(l)\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "  return t\r\n",
        "\r\n",
        "#Function to calculate gradient of ReLU activation function  \r\n",
        "def relu_grad(z):\r\n",
        "  l=[]\r\n",
        "  for i in z:\r\n",
        "    if(i[0]<0):\r\n",
        "      l.append(0)\r\n",
        "    else:\r\n",
        "      l.append(1)  \r\n",
        "  t= np.array(l)\r\n",
        "  t = t.reshape(t.shape[0],1)\r\n",
        "  return t  \r\n",
        "\r\n",
        "#Gradient of sigmoid activation function\r\n",
        "def log_grad(z):\r\n",
        "    return logistic(z)*(1-logistic(z))\r\n",
        "\r\n",
        "#Definition of tanh activation function\r\n",
        "def tanh(z):\r\n",
        "    l = []\r\n",
        "    for i in z:\r\n",
        "      if (i[0]>=600): #Handle overflows\r\n",
        "        l.append(1)\r\n",
        "      elif (i[0]<=(-600)):\r\n",
        "        l.append(-1)\r\n",
        "      else:\r\n",
        "        l.append((math.exp(i[0])-math.exp(-i[0]))/((math.exp(i[0])+math.exp(-i[0]))))\r\n",
        "        \r\n",
        "    t = np.array(l)\r\n",
        "    t = t.reshape(t.shape[0],1)\r\n",
        "    return t\r\n",
        "\r\n",
        "#Gradient of tanh activation function\r\n",
        "def tanh_grad(z):\r\n",
        "  return 1-((tanh(z))*(tanh(z)))\r\n",
        "  \r\n",
        "#Function to calculate cross-entropy loss\r\n",
        "def cross_entropy_loss(X_test,Y_test,weights,biases):\r\n",
        "    sum = 0\r\n",
        "    for i in range(Y_test.shape[0]):\r\n",
        "      y_ = feed_forward(X_test[i],logistic,weights,biases)[2]\r\n",
        "      if(Y_test[i]<y_.shape[0]):\r\n",
        "        if(y_[Y_test[i]][0] != 0):\r\n",
        "           sum = sum - math.log(y_[Y_test[i]][0])\r\n",
        "        else:\r\n",
        "           print(\"very bad\\n\")\r\n",
        "           sum = sum + 100000\r\n",
        "\r\n",
        "    return sum/Y_test.shape[0]\r\n",
        "\r\n",
        "#Function to calculate accuracy\r\n",
        "def accuracy(X_test,Y_test,weights,biases):\r\n",
        "    \r\n",
        "    sum = 0\r\n",
        "    cnt = 0\r\n",
        "\r\n",
        "    for i in range(Y_test.shape[0]):     \r\n",
        "      y_ = feed_forward(X_test[i],logistic,weights,biases)[2]\r\n",
        "      y_ = y_.flatten()\r\n",
        "      #print(y_)\r\n",
        "      j = np.argmax(y_)\r\n",
        "\r\n",
        "      if (Y_test[i] == j):\r\n",
        "        sum = sum+1\r\n",
        "\r\n",
        "    return sum/Y_test.shape[0]\r\n",
        "\r\n",
        "#Definition of softmax function\r\n",
        "def softmax(arr):    \r\n",
        "    l = []\r\n",
        "    flag = False\r\n",
        "    cnt = 0  \r\n",
        "    y = []\r\n",
        "    for i in range(arr.shape[0]) :  #Handle overflows\r\n",
        "      if(arr[i][0]<=600):\r\n",
        "         l.append(math.exp(arr[i][0]))\r\n",
        "      else:\r\n",
        "        flag = True\r\n",
        "        cnt = cnt + 1\r\n",
        "        y.append(i)\r\n",
        "\r\n",
        "    if(flag):       \r\n",
        "       temp = np.zeros(len(arr))    \r\n",
        "       for it in y:\r\n",
        "          temp[it] = 1.0/cnt;\r\n",
        "       temp = temp.reshape(temp.shape[0],1)           \r\n",
        "       return temp   \r\n",
        "         \r\n",
        "    if(np.sum(l) != 0):\r\n",
        "      l = np.array(l)\r\n",
        "      l = (l)/np.sum(l)\r\n",
        "      l = l.reshape((l.shape[0],1))\r\n",
        "      return l\r\n",
        "    else:\r\n",
        "       return (1.0/len(arr))*(np.ones(shape=(len(arr),1),dtype=float))\r\n",
        "\r\n",
        "#Helper function to perform operation on list. We decided to use store weights and biases to be able to support different size for each hidden layer easily.\r\n",
        "def upd_lst(lst1,lst2,operation,eeta): #Performs x 'operation' f(y) for each element x in lst1 and y in lst2. 'operation' - addition/subtraction/multiplication/square root of inverse\r\n",
        "   l = []\r\n",
        "   if(len(lst1)!=len(lst2)):\r\n",
        "    print(\"Lists are not of equal length\")\r\n",
        "   for (x,y) in zip(lst1,lst2):\r\n",
        "     if(operation == '+'):\r\n",
        "       l.append(x+(eeta*y))\r\n",
        "     elif(operation == '-'):\r\n",
        "       l.append(x-(eeta*y))\r\n",
        "     elif(operation == '*'):\r\n",
        "       l.append(x*y*eeta*eeta)\r\n",
        "     elif(operation == 'sqrt'):\r\n",
        "      # print(x,y,eeta)\r\n",
        "       l.append(x/np.sqrt(y+eeta))\r\n",
        "     elif(operation=='mult'):\r\n",
        "      l.append(x*eeta)\r\n",
        "\r\n",
        "   return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jriaq5o4GmyT"
      },
      "source": [
        "#Feedforward neural network\r\n",
        "def feed_forward(X,activation_function,weights,biases,output_function=softmax):\r\n",
        "    \r\n",
        "    X = X.copy()\r\n",
        "    X = X.flatten()\r\n",
        "    num_hidden_layers = len(biases)\r\n",
        "\r\n",
        "    # X is the input (x1,x2,...,xn)\r\n",
        "    # activation_function is just the activation function s.t h_i = activation_function(a_i)\r\n",
        "    # weights is a list of m*n where n is number of neurons in previous layer and m is number of neurons in current layer assume 1 based indexing \r\n",
        "    # assuming output function O is softmax function\r\n",
        "    # biases is list of  [b1,b2,..,bk] for each layer\r\n",
        "\r\n",
        "    X = X.reshape((X.shape[0],1))\r\n",
        "    a = []\r\n",
        "    h = []\r\n",
        "    \r\n",
        "    for i in range(num_hidden_layers):\r\n",
        "       if(i == 0):\r\n",
        "         a.append(np.array(0))\r\n",
        "         h.append(X)\r\n",
        "       else:        \r\n",
        "        a.append(np.matmul(weights[i],h[i-1])+biases[i])\r\n",
        "        h.append(activation_function(a[len(a)-1]))\r\n",
        "    return [a,h,output_function(a[len(a)-1])]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbZQU9bCsCJi"
      },
      "source": [
        "#Function to calculate gradients of cross entropy loss function with respect to weights and biases\r\n",
        "def calculate_gradients(y_true,y_out,h,a,weights,act_fn_derivative,weight_decay,biases):\r\n",
        "    \r\n",
        "    #delta(A_l) = -(e_ytrue - y_out)\r\n",
        "    #delta(h_i) = (W_i+1).Tdelta(A_i+1)  (for i = L-1 to 1)\r\n",
        "    #delta(A_i) = delta(h_i)*(activation_function'(A_i)) for(i = L-1 to 1)\r\n",
        "    #delta(W_k) = delta(A_k)(h_(k-1).T) for(i = L to i = 1)\r\n",
        "    #delta(b_k) = delta(A_k)\r\n",
        "\r\n",
        "    e = np.zeros(len(y_out))\r\n",
        "    e = e.reshape(e.shape[0],1)\r\n",
        "    e[y_true] = 1\r\n",
        "    A = y_out-e\r\n",
        "    l = len(h)\r\n",
        "    delta_W = []\r\n",
        "    delta_B = []\r\n",
        "    \r\n",
        "    delta_W.append(np.matmul(A,h[l-2].T))\r\n",
        "    delta_B.append(A)\r\n",
        "    l = l-2\r\n",
        "\r\n",
        "    while(l >= 1):\r\n",
        "       H = np.matmul(weights[l+1].T,A)\r\n",
        "       A = H*(act_fn_derivative(a[l]))     # assuming a is 1 based index \r\n",
        "       delta_W.append(np.matmul(A,h[l-1].T))\r\n",
        "       delta_B.append(A)\r\n",
        "       l = l-1\r\n",
        "    \r\n",
        "    delta_W.append(0)\r\n",
        "    delta_B.append(0)\r\n",
        "    delta_W.reverse()\r\n",
        "    delta_B.reverse()\r\n",
        "    delta_W=upd_lst(delta_W,weights,'+',weight_decay)\r\n",
        "    delta_B=upd_lst(delta_B,biases,'+',weight_decay)\r\n",
        "    return [delta_W,delta_B]\r\n",
        "\r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-FV-Ijsclo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "f61da6d8-0875-4772-b9ee-1acd6dc82b80"
      },
      "source": [
        "#This cell contains the definition of different optimizers\r\n",
        "\r\n",
        "#sgd optimizer\r\n",
        "def sgd(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):    \r\n",
        "    for i in range(num_epochs): \r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "            cnt = cnt + 1\r\n",
        "            r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "            a = r[0]\r\n",
        "            h = r[1]\r\n",
        "            yout = r[2]\r\n",
        "            gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "            cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "            cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "\r\n",
        "            \r\n",
        "            if(cnt == batch_size): \r\n",
        "                weights = upd_lst(weights,cum_W,'-',(learning_rate*(1.0))/batch_size)\r\n",
        "                biases = upd_lst(biases,cum_B,'-',(learning_rate*(1.0))/batch_size)\r\n",
        "                cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "                cnt = 0\r\n",
        "    return [weights,biases]\r\n",
        "\r\n",
        "#adam optimizer\r\n",
        "def adam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "    eps = 1e-8\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    for i in range(num_epochs): \r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "          m_W = cum_W\r\n",
        "          m_B = cum_B\r\n",
        "          v_W = cum_W\r\n",
        "          v_B = cum_B\r\n",
        "          a_cnt  = 0\r\n",
        "\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "             cnt = cnt + 1\r\n",
        "             r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "             cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "             cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "\r\n",
        "             if(cnt == batch_size): \r\n",
        "                 m_W = upd_lst(m_W,m_W,'+',(beta1-1))   #m_W = beta1*m_W\r\n",
        "                 m_W = upd_lst(m_W,cum_W,'+',(1-beta1)/batch_size)   #m_W += (1-beta1)*db\r\n",
        "\r\n",
        "                 m_B = upd_lst(m_B,m_B,'+',(beta1-1))   #m_B = beta1*m_B\r\n",
        "                 m_B = upd_lst(m_B,cum_B,'+',(1-beta1)/batch_size)   #m_B += (1-beta1)*db                \r\n",
        "\r\n",
        "                 d_W_sqr = upd_lst(cum_W,cum_W,'*',1.0/batch_size)\r\n",
        "                 d_B_sqr = upd_lst(cum_B,cum_B,'*',1.0/batch_size)\r\n",
        "\r\n",
        "                 v_W = upd_lst(v_W,v_W,'+',(beta2-1))  #v_W = beta2*v_W\r\n",
        "                 v_W = upd_lst(v_W,d_W_sqr,'+',(1-beta2))  #v_W += (1-beta2)*dwsqr\r\n",
        "\r\n",
        "                 v_B = upd_lst(v_B,v_B,'+',(beta2-1))  #v_B = beta2*v_B\r\n",
        "                 v_B = upd_lst(v_B,d_B_sqr,'+',(1-beta2))  #v_B += (1-beta2)*dbsqr\r\n",
        "                 \r\n",
        "                 t = math.pow(beta1,j+1)/(1-math.pow(beta1,a_cnt+1))\r\n",
        "                 m_W_hat = upd_lst(m_W,m_W,'+',t)\r\n",
        "                 m_B_hat = upd_lst(m_B,m_B,'+',t)\r\n",
        "\r\n",
        "                 t = math.pow(beta2,j+1)/(1-math.pow(beta2,a_cnt+1))\r\n",
        "                 v_W_hat = upd_lst(v_W,v_W,'+',t)\r\n",
        "                 v_B_hat = upd_lst(v_B,v_B,'+',t)\r\n",
        "\r\n",
        "                 \r\n",
        "                 m_W_hat = upd_lst(m_W_hat,v_W_hat,'sqrt',eps) #m_W_hat = m_W_hat/(sqrt(v_W_hat+eps))\r\n",
        "                 weights = upd_lst(weights,m_W_hat,'-',learning_rate)   #weights = weights - (learning rate)*m_W_hat/sqrt(v_W_hat+eps)\r\n",
        "\r\n",
        "                 m_B_hat = upd_lst(m_B_hat,v_B_hat,'sqrt',eps) #m_B_hat = m_B_hat/sqrt(V_B_hat+eps)\r\n",
        "                 biases = upd_lst(biases,m_B_hat,'-',learning_rate) #biases = biases - (learning rate)*m_B_hat/sqrt(v_B_hat+eps)\r\n",
        "\r\n",
        "                 cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                 cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "                 cnt = 0\r\n",
        "                 a_cnt = a_cnt + 1\r\n",
        "\r\n",
        "    return [weights,biases]\r\n",
        "\r\n",
        "#nadam optimizer\r\n",
        "def nadam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "    \r\n",
        "    eps = 1e-8\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    for i in range(num_epochs): \r\n",
        "\r\n",
        "          cnt = 0\r\n",
        "          cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "          cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "          m_W = cum_W\r\n",
        "          m_B = cum_B\r\n",
        "\r\n",
        "          v_W = cum_W\r\n",
        "          v_B = cum_B\r\n",
        "\r\n",
        "          cum_W_m = cum_W\r\n",
        "          cum_B_m = cum_B\r\n",
        "\r\n",
        "          a_cnt  = 0\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "             cnt = cnt + 1\r\n",
        "             r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "             cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "             cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "             \r\n",
        "             weights_m = upd_lst(weights,m_W,'-',beta1)\r\n",
        "             biases_m = upd_lst(weights,m_B,'-',beta1)\r\n",
        "             \r\n",
        "             r = feed_forward(X[j],activation_function,weights_m,biases_m)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "\r\n",
        "             gradients = calculate_gradients(Y[j],yout,h,a,weights_m,act_fn_derivative,weight_decay,biases)\r\n",
        "\r\n",
        "             cum_W_m = upd_lst(cum_W_m,gradients[0],'+',1)\r\n",
        "             cum_B_m = upd_lst(cum_B_m,gradients[1],'+',1)\r\n",
        "             \r\n",
        "\r\n",
        "             if(cnt == batch_size):\r\n",
        "                 \r\n",
        "                #  cum_W_m = upd_lst(cum_W_m,weights,'-',(weight_decay)*batch_size)\r\n",
        "                #  cum_B_m = upd_lst(cum_B_m,biases,'-',(weight_decay)*batch_size)\r\n",
        "\r\n",
        "                #  cum_W = upd_lst(cum_W,weights,'-',(weight_decay)*batch_size)\r\n",
        "                #  cum_B = upd_lst(cum_B,biases,'-',(weight_decay)*batch_size)\r\n",
        "                 \r\n",
        "                 m_W = upd_lst(m_W,m_W,'+',(beta1-1))   #m_W = beta1*m_W\r\n",
        "                 m_W = upd_lst(m_W,cum_W_m,'+',(1-beta1)/batch_size)   #m_W += (1-beta1)*db\r\n",
        "\r\n",
        "                 m_B = upd_lst(m_B,m_B,'+',(beta1-1))   #m_B = beta1*m_B\r\n",
        "                 m_B = upd_lst(m_B,cum_B_m,'+',(1-beta1)/batch_size)   #m_B += (1-beta1)*db\r\n",
        "                 \r\n",
        "\r\n",
        "                 d_W_sqr = upd_lst(cum_W,cum_W,'*',1.0/batch_size)\r\n",
        "                 d_B_sqr = upd_lst(cum_B,cum_B,'*',1.0/batch_size)\r\n",
        "\r\n",
        "                 v_W = upd_lst(v_W,v_W,'+',(beta2-1))  #v_W = beta2*v_W\r\n",
        "                 v_W = upd_lst(v_W,d_W_sqr,'+',(1-beta2))  #v_W += (1-beta2)*dwsqr\r\n",
        "\r\n",
        "                 v_B = upd_lst(v_B,v_B,'+',(beta2-1))  #v_B = beta2*v_B\r\n",
        "                 v_B = upd_lst(v_B,d_B_sqr,'+',(1-beta2))  #v_B += (1-beta2)*dbsqr\r\n",
        "                 \r\n",
        "                 t = math.pow(beta1,j+1)/(1-math.pow(beta1,a_cnt+1))\r\n",
        "                 m_W_hat = upd_lst(m_W,m_W,'+',t)\r\n",
        "                 m_B_hat = upd_lst(m_B,m_B,'+',t)\r\n",
        "\r\n",
        "                 t = math.pow(beta2,j+1)/(1-math.pow(beta2,a_cnt+1))\r\n",
        "                # print(beta2,math.pow(beta2,a_cnt+1))\r\n",
        "                 v_W_hat = upd_lst(v_W,v_W,'+',t)\r\n",
        "                 v_B_hat = upd_lst(v_B,v_B,'+',t)\r\n",
        "\r\n",
        "                 \r\n",
        "                 m_W_hat = upd_lst(m_W_hat,v_W_hat,'sqrt',eps) #m_W_hat = m_W_hat/(sqrt(v_W_hat+eps))\r\n",
        "                 weights = upd_lst(weights,m_W_hat,'-',learning_rate)   #weights = weights - (learning rate)*m_W_hat/sqrt(v_W_hat+eps)\r\n",
        "\r\n",
        "                 m_B_hat = upd_lst(m_B_hat,v_B_hat,'sqrt',eps) #m_B_hat = m_B_hat/sqrt(V_B_hat+eps)\r\n",
        "                 biases = upd_lst(biases,m_B_hat,'-',learning_rate) #biases = biases - (learning rate)*m_B_hat/sqrt(v_B_hat+eps)\r\n",
        "\r\n",
        "                 cum_W = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "                 cum_B = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "                 cum_W_m = cum_W\r\n",
        "                 cum_B_m = cum_B\r\n",
        "\r\n",
        "                 cnt = 0\r\n",
        "                 a_cnt = a_cnt + 1\r\n",
        "\r\n",
        "\r\n",
        "    return [weights,biases]\r\n",
        " \r\n",
        " #momentum optimizer\r\n",
        " def momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(weight)\r\n",
        "  #weigt{t}=weight{t-1}-update{t}\r\n",
        "\r\n",
        "  gamma=0.9 # The typical value of gamma used is 0.9\r\n",
        "  for lmao in range(num_epochs):\r\n",
        "    #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "    weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "    #Initialisation of update_weights\r\n",
        "    update_weights=weights_grad_tot\r\n",
        "    update_biases=biases_grad_tot\r\n",
        "\r\n",
        "    print(lmao)\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "    dum=0\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "      y_pred=feed_forward(x,activation_function,weights,biases)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        \r\n",
        "        update_weights = upd_lst(update_weights,update_weights,'+',gamma-1)\r\n",
        "        update_biases = upd_lst(update_biases,update_biases,'+',gamma-1)\r\n",
        "        update_weights=upd_lst(update_weights,weights_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        update_biases=upd_lst(update_biases,biases_grad_tot,'+',learning_rate/batch_size)\r\n",
        "       \r\n",
        "        weights=upd_lst(weights,update_weights,'-',1.0)\r\n",
        "        biases=upd_lst(biases,update_biases,'-',1.0)\r\n",
        "        num_data_point_done=0\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]\r\n",
        "\r\n",
        "#nesterov optimiser \r\n",
        "def nesterov(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "\r\n",
        "  gamma=0.9 # The typical value of gamma used is 0.9\r\n",
        "\r\n",
        "  #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "  weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "  biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "\r\n",
        "  #Initialisation of update_weights\r\n",
        "  update_weights=weights_grad_tot\r\n",
        "  update_biases=biases_grad_tot  \r\n",
        "  for lmao in range(num_epochs):\r\n",
        "\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "    w_ahead=weights # Initial value of w_ahead is weights\r\n",
        "    b_ahead=biases\r\n",
        "\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "      y_pred=feed_forward(x,activation_function,w_ahead,b_ahead)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,w_ahead,act_fn_derivative,weight_decay,b_ahead)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)   \r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        # weights_grad_tot = upd_lst(weights_grad_tot,weights,'-',(weight_decay)*batch_size)\r\n",
        "        # biases_grad_tot = upd_lst(biases_grad_tot,biases,'-',(weight_decay)*batch_size)\r\n",
        "\r\n",
        "        w_ahead=upd_lst(weights,update_weights,'-',gamma)\r\n",
        "        b_ahead=upd_lst(biases,update_biases,'-',gamma)\r\n",
        "\r\n",
        "        update_weights = upd_lst(update_weights,update_weights,'+',gamma-1)\r\n",
        "        update_biases = upd_lst(update_biases,update_biases,'+',gamma-1)\r\n",
        "\r\n",
        "        update_weights=upd_lst(update_weights,weights_grad_tot,'+',learning_rate/batch_size)\r\n",
        "        update_biases=upd_lst(update_biases,biases_grad_tot,'+',learning_rate/batch_size)\r\n",
        "\r\n",
        "        # update_weights = update_weights + gamma*update_weights + learning_rate * weights_grad_tot\r\n",
        "        # update_biases = update_biases + gamma*update_weight + learning_rate * biases_grad_tot\r\n",
        "\r\n",
        "        weights=upd_lst(weights,update_weights,'-',1.0)\r\n",
        "        biases=upd_lst(biases,update_biases,'-',1.0)\r\n",
        "        num_data_point_done=0\r\n",
        "\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]\r\n",
        "\r\n",
        "\r\n",
        "#This is additional attempt to implement adagrad optimizer(Note that the definition is not complete) \r\n",
        "# def adagrad(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "#   #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "#   #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "#   # w_ahead= weights\r\n",
        "#   # b_ahead=biases\r\n",
        "#   for i in range(num_epochs):\r\n",
        "#     # num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "#     Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "#     weights_grad_tot=[]\r\n",
        "#     for w in weights:\r\n",
        "#       weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "#     biases_grad_tot=[]\r\n",
        "#     for b in biases:\r\n",
        "#       biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "#     num_data_point_done=0\r\n",
        "#     v_weight = 0 #Intitial value of 0\r\n",
        "#     v_bias=0\r\n",
        "#     epsilon = 0 #Figure out what is epsilon\r\n",
        "#     for x,y,y_pred in zip(X,Y,Y_pred):    \r\n",
        "#       [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,weights,act_fn_derivative)\r\n",
        "#       weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "#       biases_grad_tot = biases_grad_tot + biases_grad\r\n",
        "#       num_data_point_done=num_data_point_done+1\r\n",
        "#       if(num_data_point_done==batch_size):\r\n",
        "#         v_weight = v_weight + (weights_grad_total)**2\r\n",
        "#         v_bias = v_bias + (biases_grad_total)**2\r\n",
        "#         v_weight_dum = learning_rate/np.sqrt(v_weight+epsilon)\r\n",
        "#         v_bias_dum = learning_rate/np.sqrt(v_bias+epsilon)\r\n",
        "#         weights=weights- v_weight_dum * weights_grad_tot\r\n",
        "#         biases=biases - v_bias_dum * bias_grad_tot\r\n",
        "#         num_data_point_done=0\r\n",
        "\r\n",
        "#rmsprop optimizer  \r\n",
        "def rmsprop(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "  beta=0.9 # The typical value of beta used is 0.9\r\n",
        "  epsilon = 1e-8  # Typical value of epsilon used\r\n",
        "\r\n",
        "  for lmao in range(num_epochs):\r\n",
        "\r\n",
        "    #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "    weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "    v_weight = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "    b_weight = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]   \r\n",
        "\r\n",
        "    # num_hidden_layers=len(biases) #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list    \r\n",
        "    num_data_point_done=0\r\n",
        "\r\n",
        "    for x,y in zip(X,Y):  \r\n",
        "    \r\n",
        "      y_pred=feed_forward(x,activation_function,weights,biases)  #Find prediction for each data instance of the data set\r\n",
        "      a = y_pred[0]\r\n",
        "      h = y_pred[1]\r\n",
        "      yout = y_pred[2]\r\n",
        "\r\n",
        "      [weights_grad,biases_grad] =  calculate_gradients(y,yout,h,a,weights,act_fn_derivative,weight_decay,biases)\r\n",
        "      weights_grad_tot = upd_lst(weights_grad_tot,weights_grad,'+',1)\r\n",
        "      biases_grad_tot = upd_lst(biases_grad_tot,biases_grad,'+',1)\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "\r\n",
        "      if(num_data_point_done==batch_size):   \r\n",
        "        # weights_grad_tot = upd_lst(weights_grad_tot,weights,'-',(weight_decay)*batch_size)\r\n",
        "        # biases_grad_tot = upd_lst(biases_grad_tot,biases,'-',(weight_decay)*batch_size)\r\n",
        "\r\n",
        "        v_weight = upd_lst(v_weight,v_weight,'+',(beta-1))\r\n",
        "        dum1=upd_lst(weights_grad_tot,weights_grad_tot,'*',1.0/batch_size)\r\n",
        "\r\n",
        "        v_weight= upd_lst(v_weight,dum1,'+',(1-beta))\r\n",
        "        b_weight = upd_lst(b_weight,b_weight,'+',(beta-1))\r\n",
        "\r\n",
        "        dum2=upd_lst(biases_grad_tot,biases_grad_tot,'*',1.0/batch_size)\r\n",
        "        b_weight= upd_lst(b_weight,dum2,'+',(1-beta))\r\n",
        "\r\n",
        "        weights_grad_tot=upd_lst(weights_grad_tot,weights_grad_tot,'mult',1.0/batch_size)\r\n",
        "        v_weight_dum=upd_lst(weights_grad_tot,v_weight,'sqrt',epsilon)\r\n",
        "        weights=upd_lst(weights, v_weight_dum,'-',learning_rate)\r\n",
        "        biases_grad_tot=upd_lst(biases_grad_tot,biases_grad_tot,'mult',1.0/batch_size)\r\n",
        "\r\n",
        "        b_weight_dum=upd_lst(biases_grad_tot,b_weight,'sqrt',epsilon)\r\n",
        "        biases=upd_lst(biases, b_weight_dum,'-',learning_rate)       \r\n",
        "        num_data_point_done=0\r\n",
        "\r\n",
        "        weights_grad_tot = [np.zeros(weights[i].shape) if(i != 0) else 0 for i in range(len(weights))]\r\n",
        "        biases_grad_tot = [np.zeros(biases[i].shape) if(i != 0) else 0 for i in range(len(biases))]\r\n",
        "  return [weights,biases]        \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c44ec1b5e871>\"\u001b[0;36m, line \u001b[0;32m195\u001b[0m\n\u001b[0;31m    def momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\u001b[0m\n\u001b[0m                                                                                                                            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuklB5S78jQy"
      },
      "source": [
        "#Definition of the backpropogation framework\r\n",
        "def backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay,validateX,validatey):\r\n",
        "    \r\n",
        "    \r\n",
        "    weights = [0]  #dummy value\r\n",
        "    biases =  [0]  #dummy value \r\n",
        "    \r\n",
        "    #Initiliase activation function and gradient of activation function\r\n",
        "    if(activation=='sigmoid'):\r\n",
        "      activation_function=logistic\r\n",
        "      act_fn_derivative=log_grad\r\n",
        "    elif(activation=='tanh'):\r\n",
        "      activation_function=tanh\r\n",
        "      act_fn_derivative=tanh_grad\r\n",
        "    elif(activation=='ReLU'):\r\n",
        "      activation_function=relu\r\n",
        "      act_fn_derivative=relu_grad\r\n",
        "\r\n",
        "    #Weight initialisation\r\n",
        "    if (weight_initialisation == 'random'):        \r\n",
        "        num_classes = len(np.unique(Y))\r\n",
        "        i = len(X[0].flatten())\r\n",
        "        a = -1\r\n",
        "        b = 1\r\n",
        "\r\n",
        "        for j in range(num_hidden_layers-1):\r\n",
        "            weights.append((b-a)*np.random.rand(sz_hidden_layer,i)+a)\r\n",
        "            biases.append(np.zeros((sz_hidden_layer,1)))\r\n",
        "            i = sz_hidden_layer\r\n",
        "        weights.append((b-a)*np.random.rand(num_classes,i)+a)\r\n",
        "        biases.append(np.zeros((num_classes,1)))\r\n",
        "\r\n",
        "    else:\r\n",
        "\r\n",
        "        #Xavier initilization\r\n",
        "        num_classes = len(np.unique(Y))\r\n",
        "        i = len(X[0].flatten())\r\n",
        "\r\n",
        "        for j in range(num_hidden_layers-1):\r\n",
        "            t = math.sqrt(6)/math.sqrt(i + sz_hidden_layer)\r\n",
        "            weights.append(np.random.uniform(-t,t,(sz_hidden_layer,i)))\r\n",
        "            biases.append(np.zeros((sz_hidden_layer,1)))\r\n",
        "            i = sz_hidden_layer\r\n",
        "        \r\n",
        "        t = math.sqrt(6)/math.sqrt(i + num_classes)\r\n",
        "        weights.append(np.random.uniform(-t,t,(num_classes,i)))\r\n",
        "        biases.append(np.zeros((num_classes,1)))\r\n",
        "\r\n",
        "    #Use the appropriate optimiser\r\n",
        "    fun = 1\r\n",
        "    if (optimizer == 'sgd'):\r\n",
        "        fun = sgd\r\n",
        "    elif (optimizer == 'momentum'):\r\n",
        "        fun = momentum\r\n",
        "    elif (optimizer == 'nesterov'):\r\n",
        "        fun = nesterov\r\n",
        "    elif (optimizer == 'rmsprop'):\r\n",
        "        fun = rmsprop\r\n",
        "    elif (optimizer == 'adam'):\r\n",
        "        fun = adam\r\n",
        "    elif (optimizer == 'nadam'):\r\n",
        "        fun = nadam\r\n",
        "    \r\n",
        "    xvalues = []\r\n",
        "    yvalues = []\r\n",
        "    \r\n",
        "    for i in range(num_epochs):\r\n",
        "      [weights,biases] = fun(X,Y,1,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative) #backpropagation(trainX,trainy,1,config.num_hidden_layer,config.size_layer,config.learning_rate,config.optimizer,config.batch_size,config.weight_intialisation,config.activation,config.weight_decay)\r\n",
        "      print('val_accuracy'+str(accuracy(validateX,validatey,weights,biases)))\r\n",
        "      print('val_loss'+str(cross_entropy_loss(validateX,validatey,weights,biases)))\r\n",
        "      print('accuracy'+str(accuracy(X,Y,weights,biases)))\r\n",
        "      print('loss'+str(cross_entropy_loss(X,Y,weights,biases)))\r\n",
        "      # wandb.log({'epoch':i})\r\n",
        "      # wandb.log({'val_accuracy':accuracy(validateX,validatey,weights,biases)})\r\n",
        "      # wandb.log({'val_loss':cross_entropy_loss(validateX,validatey,weights,biases)})\r\n",
        "      # wandb.log({'accuracy':accuracy(X,Y,weights,biases)})\r\n",
        "      # wandb.log({'loss':cross_entropy_loss(X,Y,weights,biases)})\r\n",
        "      # wandb.log({'epoch':i})\r\n",
        "    return [weights,biases]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKnNVKLFKZuW"
      },
      "source": [
        "#This cell contains commented code that was used to debug our code\r\n",
        "\r\n",
        "# import warnings\r\n",
        "# warnings.simplefilter('error', RuntimeWarning) # To help debug where the warnings are\r\n",
        "#backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay):\r\n",
        "#[weights,biases] = backpropagation(trainX,trainy,5,3,32,1e-3,'sgd',16,'random','sigmoid',0) #logistic,log_grad)\r\n",
        "# # [weights_momentum,biases_momentum] = backpropagation(trainX/255,trainy,5,3,32,0,1e-3,'momentum',16,'random',logistic,log_grad)\r\n",
        "#def backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay,validateX,validatey):\r\n",
        "# backpropagation(trainX,trainy,5,3,32,1e-3,'nesterov',64,'random','sigmoid',0.0005,validateX,validatey)\r\n",
        "# #[weights_rmsprop,biases_rmsprop] = backpropagation(trainX/255,trainy,5,3,32,0,1e-3,'rmsprop',16,'random',logistic,log_grad)\r\n",
        "# #t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# # t_cross_momentum = cross_entropy_loss(testX, testy,weights_momentum,biases_momentum)\r\n",
        "# #t_cross_nestrov = cross_entropy_loss(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# # t_cross_rmsprop = cross_entropy_loss(testX, testy,weights_rmsprop,biases_rmsprop)\r\n",
        "\r\n",
        "# #print(t)\r\n",
        "# # print(t_cross_momentum)\r\n",
        "# #print(t_cross_nestrov)\r\n",
        "# print(t_cross_rmsprop)\r\n",
        "\r\n",
        "# #t = accuracy(testX, testy,weights,biases)\r\n",
        "# # t_momentum=accuracy(testX, testy,weights_momentum,biases_momentum)\r\n",
        "# #t_nestrov=accuracy(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# t_rmsprop=accuracy(testX, testy,weights_rmsprop,biases_rmsprop)\r\n",
        "# #print(t)\r\n",
        "# # # print(t_momentum)\r\n",
        "# # print(t_rmsprop)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,learning_rate,optimizer,batch_size,weight_initialisation,activation,weight_decay,validateX,validatey):\r\n",
        "\r\n",
        "# [weights,biases] = backpropagation(trainX,trainy,10,3,64,0.001,'adam',64,'random','sigmoid',0.0005,validateX,validatey)\r\n",
        "\r\n",
        "# t = cross_entropy_loss(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# print(t)\r\n",
        "# t = accuracy(testX, testy,weights_nestrov,biases_nestrov)\r\n",
        "# print(t)\r\n",
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,0,1e-3,'sgd',16,'xavier',logistic,log_grad)\r\n",
        "# t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# print(t)\r\n",
        "\r\n",
        "# t = accuracy(testX, testy,weights,biases)\r\n",
        "# print(t)\r\n",
        "\r\n",
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,0,1e-3,'sgd',16,'xavier',tanh,tanh_grad)\r\n",
        "# t = cross_entropy_loss(testX, testy,weights,biases)\r\n",
        "# print(t)\r\n",
        "# t = accuracy(testX, testy,weights,biases)\r\n",
        "# print(t)\r\n",
        "\r\n",
        "#The recommended set of parameters\r\n",
        "# num_epochs = [5,10,15]\r\n",
        "\r\n",
        "# weight_decay = [0,0.0005,0.5]\r\n",
        "# learning_rate = [1e-3,1e-4]\r\n",
        "# optimiser = ['sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam']\r\n",
        "# batch_size =[16, 32, 64]\r\n",
        "\r\n",
        "# activation= ['sigmoid', 'tanh', 'ReLU']\r\n",
        "\r\n",
        "# weight_initialisation = [ 'random', 'Xavier']\r\n",
        "# num_hidden_layer = [3,4,5]\r\n",
        "# size_layer = [32,64,128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFuD9Bu7ojO5"
      },
      "source": [
        "#Set the configuration for wandb sweep\r\n",
        "sweep_config = {\r\n",
        "    'name': 'max_accuracy_1.2',\r\n",
        "    'method' : 'random',\r\n",
        "    'metric': {        \r\n",
        "         'goal': 'maximize',\r\n",
        "         'name' : 'val_accuracy'\r\n",
        "    },\r\n",
        "    'parameters': {\r\n",
        "        'epochs':{\r\n",
        "            'values': [10]\r\n",
        "        },\r\n",
        "        'batch_size':{\r\n",
        "            'values': [64]\r\n",
        "        },\r\n",
        "        'weight_decay':{\r\n",
        "            'values': [0.0005]\r\n",
        "        },\r\n",
        "        'learning_rate':{\r\n",
        "            'values': [0.001]\r\n",
        "        },\r\n",
        "        'optimizer':{\r\n",
        "            'values': ['adam']\r\n",
        "        },\r\n",
        "        'activation':{\r\n",
        "            'values': ['sigmoid']\r\n",
        "        },\r\n",
        "        'weight_intialisation':{\r\n",
        "            'values': ['random']\r\n",
        "        },\r\n",
        "        'num_hidden_layer': {\r\n",
        "            'values': [5]\r\n",
        "        },\r\n",
        "        'size_layer':{\r\n",
        "            'values': [128]\r\n",
        "        }\r\n",
        "\r\n",
        "    }\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1G4h-4HOvpu"
      },
      "source": [
        "#The function which is to be sweeped by wandb\r\n",
        "def train():\r\n",
        "  config_default = {\r\n",
        "        'epochs': 10,\r\n",
        "        'batch_size': 64,\r\n",
        "        'weight_decay':0.0005, \r\n",
        "        'learning_rate':0.001,\r\n",
        "        'optimizer': 'adam',\r\n",
        "        'activation': 'sigmoid', \r\n",
        "        'weight_intialisation':'random', \r\n",
        "        'num_hidden_layer': 5,\r\n",
        "        'size_layer': 128\r\n",
        "  }\r\n",
        "  print(\"wand about to be initialised\")\r\n",
        "  #config=wandb.config\r\n",
        "  #name = 'epochs_'+str(config.epochs)+'_btch_sz_'+str(config.batch_size)+'_wght_dcy_'+str(config.weight_decay)+'_optm_'+config.optimizer+'_lrng_rat_'+config.learning_rate+'_actvn_'+config.activation+'_numhdnlyr_'+str(config.num_hidden_layer)+' sz_lyr_'+str(config.size_layer)\r\n",
        "  wandb.init(config=config_default,project='cs6910-assignment1')\r\n",
        "  config = wandb.config\r\n",
        "  a = trainX\r\n",
        "  b = trainy\r\n",
        "  \r\n",
        "  backpropagation(trainX,trainy,config.epochs,config.num_hidden_layer,config.size_layer,config.learning_rate,config.optimizer,config.batch_size,config.weight_intialisation,config.activation,config.weight_decay,validateX,validatey) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FtTIclCSSCE"
      },
      "source": [
        "#Commented code used to check our code\r\n",
        "\r\n",
        "#agent=wandb.init(entity=\"nvvishnu\",project=\"cs6910-assignment1\")\r\n",
        "# sweep_config['parameters']['epochs']\r\n",
        "# # # import warnings\r\n",
        "# # # warnings.simplefilter('error', RuntimeWarning) # To help debug where the warnings are"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJRqfvgM-O-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf787a5d-9add-4fdd-e036-4473d3d90376"
      },
      "source": [
        "#Create a sweep and generate sweep id\r\n",
        "sweep_id = wandb.sweep(sweep_config,entity=\"nvvishnu\",project=\"cs6910-assignment1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: vb52k0il\n",
            "Sweep URL: https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq70j4ZKT3TC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbf4ce3667d243689f2a3149a146fe76",
            "47c9f9b4911a4d60a22a6a6f9d4f2422",
            "dd1b93d89a71420c95886611f03e8ac4",
            "de0e9bd86b124fd9a8e3934a0a52d440",
            "573bd6efba9c4f0faeabc09149be366c",
            "9d59b9ce1299431da8640288100bed8a",
            "f78b13f25165442d91354605acf682ba",
            "d292200ee4d64960b94fe165bbb6aa98"
          ]
        },
        "outputId": "4d41fcfd-74d7-464e-8e05-31dd6354e0e3"
      },
      "source": [
        "#Run wandb agent to sweep\r\n",
        "wandb.agent(sweep_id,project=\"cs6910-assignment1\",function=train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ot4gebxl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_intialisation: random\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wand about to be initialised\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">snowy-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_100131-ot4gebxl</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 871<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf4ce3667d243689f2a3149a146fe76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210313_100131-ot4gebxl/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210313_100131-ot4gebxl/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>9</td></tr><tr><td>_runtime</td><td>7360</td></tr><tr><td>_timestamp</td><td>1615637051</td></tr><tr><td>_step</td><td>59</td></tr><tr><td>val_accuracy</td><td>0.8635</td></tr><tr><td>val_loss</td><td>0.38837</td></tr><tr><td>accuracy</td><td>0.87815</td></tr><tr><td>loss</td><td>0.34936</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅▆▆▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr><tr><td>accuracy</td><td>▁▃▃▄▅▆▆▇██</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▂▂▁▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">snowy-sweep-1</strong>: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/ot4gebxl</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5trei2ti with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layer: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsize_layer: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_intialisation: random\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wand about to be initialised\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">jolly-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/sweeps/vb52k0il</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/nvvishnu/cs6910-assignment1/runs/5trei2ti\" target=\"_blank\">https://wandb.ai/nvvishnu/cs6910-assignment1/runs/5trei2ti</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210313_120416-5trei2ti</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 926<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQ55FntkHc1"
      },
      "source": [
        "#Definition of function to plot confusion matrix\r\n",
        "def confusion_matrix(X_test,Y_test,weights,biases):\r\n",
        "\r\n",
        "    conf_mtrx = np.zeros(shape=(10,10))\r\n",
        "\r\n",
        "    for i in range(Y_test.shape[0]):\r\n",
        "\r\n",
        "        y_ = feed_forward(X_test[i],logistic,weights,biases)[2]\r\n",
        "        y_ = y_.flatten()\r\n",
        "        #print(y_)\r\n",
        "        j = np.argmax(y_)\r\n",
        "        conf_mtrx[Y_test[i]][j] = conf_mtrx[Y_test[i]][j] + 1\r\n",
        "    \r\n",
        "    return conf_mtrx\r\n",
        "    \r\n",
        "# [weights,biases] = backpropagation(trainX,trainy,5,3,32,1e-3,'sgd',16,'random','sigmoid',0)\r\n",
        "conf_mtrx = confusion_matrix(testX, testy,weights,biases)\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "3cUO2qmuxZU3",
        "outputId": "ac54f89d-ae61-4338-f088-9ac4f5d61131"
      },
      "source": [
        "#Generate heatmap for confusion matrix\r\n",
        "\r\n",
        "names = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\r\n",
        "# confusion_plot=pyplot.axes()\r\n",
        "confusion_matrix=sns.heatmap(conf_mtrx.transpose(),linecolor= 'white', cbar=True,cmap=\"YlGnBu\",annot=True,fmt=\".0f\",xticklabels=names,yticklabels=names)\r\n",
        "pyplot.xlabel('Predicted Labels')\r\n",
        "pyplot.ylabel('True Labels')\r\n",
        "pyplot.title('Confusion Matrix')\r\n",
        "pyplot.tight_layout()\r\n",
        "# confusion_matrix=pyplot.show()\r\n",
        "pyplot.savefig('confustion_matrix.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUgfgJSL1pKB",
        "outputId": "6cb6d6f7-d412-49e9-ec4b-18c9b20e0378"
      },
      "source": [
        "#Print the test accuracy for the best model\r\n",
        "cnt=0\r\n",
        "for i in range(10):\r\n",
        "  cnt+=conf_mtrx[i][i]\r\n",
        "print(cnt/testy.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrExw29p9vbW",
        "outputId": "7e8bfb74-25f6-445c-b32f-f2af82ad2c37"
      },
      "source": [
        "#Used to store the optimal weights and biases so that they can re-used when runtime is reconnected\r\n",
        "!pip install pickle-mixin\r\n",
        "import pickle\r\n",
        "weight_name = \"weights.pkl\"\r\n",
        "open_file = open(weight_name, \"wb\")\r\n",
        "pickle.dump(weights, open_file)\r\n",
        "open_file.close()\r\n",
        "biases_name = \"biases.pkl\"\r\n",
        "open_file = open(biases_name, \"wb\")\r\n",
        "pickle.dump(biases, open_file)\r\n",
        "open_file.close()\r\n",
        "open_file = open(weight_name, \"rb\")\r\n",
        "weight_dum = pickle.load(open_file)\r\n",
        "open_file.close()\r\n",
        "open_file = open(biases_name, \"rb\")\r\n",
        "biases_dum = pickle.load(open_file)\r\n",
        "open_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle-mixin\n",
            "  Downloading https://files.pythonhosted.org/packages/02/77/9d5eb2201bbc130e2a5cc41fc949e4ab0da74b619107eac1c511be3af7a7/pickle-mixin-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-cp37-none-any.whl size=5999 sha256=7f4489749e3e419cd4503fc2c5fc2ba814b91a3bf5ae5cc6f2c2dc335a484d51\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/05/42/71de70fa36b9cbb7657bb5793a16f8028c1cdc1bdd3b8e1ac3\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin\n",
            "Successfully installed pickle-mixin-1.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju5QfUsE8XdX"
      },
      "source": [
        "#Commented Code to check our code\r\n",
        "\r\n",
        "# conf_mtrx_dum = confusion_matrix(testX, testy,weight_dum,biases_dum)\r\n",
        "# cnt=0\r\n",
        "# for i in range(10):\r\n",
        "#   cnt+=conf_mtrx_dum[i][i]\r\n",
        "print(cnt/testy.shape[0])\r\n",
        "# with open(\"weights.txt\", \"w\") as f:\r\n",
        "#     for s in :\r\n",
        "#         f.write(str(s) +\"\\n\")\r\n",
        "# with open(\"biases.txt\", \"w\") as f:\r\n",
        "#     for s in biases:\r\n",
        "#         f.write(str(s) +\"\\n\")\r\n",
        "# weight_dum=[]\r\n",
        "# with open(\"weights.txt\", \"r\") as f:\r\n",
        "#   for line in f:\r\n",
        "#     weight_dum.append((line.strip()))\r\n",
        "# # weight_dum=weight_dum.astype(np.float)\r\n",
        "# biases_dum=[]\r\n",
        "# with open(\"file.txt\", \"r\") as f:\r\n",
        "#   for line in f:\r\n",
        "#     biases_dum.append(float(line.strip()))\r\n",
        "# print(weight_dum)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}