{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignement1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvvishnu/CS6910-Assignments/blob/main/Assignement1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2jhbImHGSyM",
        "outputId": "93265164-754e-4df1-c0c4-2a34df990f84"
      },
      "source": [
        "!pip install wandb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import wandb\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 10.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.5MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 53.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=42f94715318b78da0326e91c76c8450a626c921a8bd25207376121403a6eda76\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=92a5708117d21b166e44e5637c561e407a2736c4227fda6117053b247f4e3e90\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, subprocess32, configparser, sentry-sdk, shortuuid, pathtools, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1OfVTIUo_g5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b651355-4cb8-4fd7-cbe6-f8e1a1235c78"
      },
      "source": [
        "(trainX, trainy), (testX, testy) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkn-Go21pMdQ",
        "outputId": "f008c6aa-c833-4fcb-df19-21afa2bf2148"
      },
      "source": [
        "np.unique(trainy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsS6iYFsq2h9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a074a6f5-cc7e-453d-bc9e-62521ff5d116"
      },
      "source": [
        "trainy.T.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "r1RDYJJUq9z7",
        "outputId": "b89194a5-08b8-44e8-839a-64fe575ab3f2"
      },
      "source": [
        "np.exp(700)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-88be095c2278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jriaq5o4GmyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "dcb74018-0fe2-447b-b55d-80f31d425942"
      },
      "source": [
        "\r\n",
        "def softmax(arr):\r\n",
        "\r\n",
        "    l = []\r\n",
        "\r\n",
        "    flag = False\r\n",
        "\r\n",
        "    cnt = 0\r\n",
        "\r\n",
        "    y = []\r\n",
        "    for i in arr : \r\n",
        "      if(i<=700):\r\n",
        "         l.append(np.exp(i))\r\n",
        "      else:\r\n",
        "        flag = True\r\n",
        "        cnt = cnt + 1\r\n",
        "        y.append(i)\r\n",
        "\r\n",
        "    if(flag):\r\n",
        "\r\n",
        "       temp = np.zeros(shape=(len(arr),1),dtype=float)\r\n",
        "       for it in y:\r\n",
        "          temp[it][0] = 1.0/cnt;\r\n",
        "\r\n",
        "       return temp   \r\n",
        "         \r\n",
        "    if(np.sum(l) != 0):\r\n",
        "      l = np.array(l)\r\n",
        "      l = (l)/np.sum(l)\r\n",
        "\r\n",
        "      l = l.reshape((l.shape[0],1))\r\n",
        "\r\n",
        "      return l\r\n",
        "\r\n",
        "    else:\r\n",
        "       return (1.0/len(arr))*(np.ones(shape=(len(arr),1),dtype=float))\r\n",
        "\r\n",
        "\r\n",
        "def feed_forward(X,activation_function,weights,biases,output_function=softmax):\r\n",
        "\r\n",
        "    X = X.flatten()\r\n",
        "    num_hidden_layers = len(biases)\r\n",
        "    # X is the input (x1,x2,...,xn)\r\n",
        "    # activation_function is just the activation function s.t h_i = activation_function(a_i)\r\n",
        "    # weights is a list of m*n where n is number of neurons in previous layer and m is number of neurons in current layer assume 1 based indexing \r\n",
        "    # assuming output function O is softmax function\r\n",
        "    # biases is list of  [b1,b2,..,bk] for each layer\r\n",
        "\r\n",
        "    X = X.reshape((X.shape[0],1))\r\n",
        "    a = [X]\r\n",
        "    h = [X]\r\n",
        "    \r\n",
        "    #I will add checks for the num_neurons\r\n",
        "    for i in range(num_hidden_layers):\r\n",
        "       if(i == 0):\r\n",
        "         continue\r\n",
        "       # a_i = W_ih_(i-1) + b_i\r\n",
        "       a.append(np.matmul(weights[i],h[i-1])+biases[i])\r\n",
        "       h.append(activation_function(a[len(a)-1])\r\n",
        "\r\n",
        "    return [a,h,output_function(a[len(a)-1])]\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-cbd3fff36f9f>\"\u001b[0;36m, line \u001b[0;32m61\u001b[0m\n\u001b[0;31m    return [a,h,output_function(a[len(a)-1])]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baJ8wJH75-Di"
      },
      "source": [
        "#Helper functions\r\n",
        "def sigmoid(z):\r\n",
        "  return 1/(1+exp(-z))\r\n",
        "def tanh(z):\r\n",
        "  return (exp(z)-exp(-z))/(e(z)+e(-z))\r\n",
        "def sigmoid_grad(z):\r\n",
        "  return sigmoid(z)/(1-sigmoid(z))\r\n",
        "def tanh_grad(z):\r\n",
        "  return 1-(tanh(z))*(tanh(z))\r\n",
        "def relu(z):\r\n",
        "  return max(0,z)\r\n",
        "def relu_grad(z):\r\n",
        "  if(z<0):\r\n",
        "    return 0\r\n",
        "  else:\r\n",
        "    return 1\r\n",
        "def grad_output(l,y):\r\n",
        "  n=y.size()[0]\r\n",
        "  e=np.zeroes(n)\r\n",
        "  for i in range(0,n):\r\n",
        "    if(l==(i+1)):\r\n",
        "      e[i]=1\r\n",
        "  return -(e-y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbZQU9bCsCJi"
      },
      "source": [
        "def caluculate_gradients(y_true,y_out,h,a,weights,act_fn_derivative):\r\n",
        "    \r\n",
        "    #delta(A_l) = -(e_ytrue - y_out)\r\n",
        "    #delta(h_i) = (W_i+1).Tdelta(A_i+1)  (for i = L-1 to 1)\r\n",
        "    #delta(A_i) = delta(h_i)*(activation_function'(A_i)) for(i = L-1 to 1)\r\n",
        "    #delta(W_k) = delta(A_k)(h_(k-1).T) for(i = L to i = 1)\r\n",
        "    #delta(b_k) = delta(A_k)\r\n",
        "\r\n",
        "\r\n",
        "    e = np.zeros(len(y_out))\r\n",
        "    e[y_true] = 1\r\n",
        "    A = y_out-e\r\n",
        "    l = len(y_out)\r\n",
        "    delta_W = []\r\n",
        "    delta_W.append(np.matmul(A,h[l-1].T))\r\n",
        "    delta_B.append(A)\r\n",
        "    l = l-1\r\n",
        "    while(l >= 1):\r\n",
        "\r\n",
        "       H = np.matmul(weights[l+1].T,A)\r\n",
        "       A = H*(act_fn_derivative(a[l]))     # assuming a is 1 based index \r\n",
        "       delta_W.append(np.matmul(A,h[l-1].T))\r\n",
        "       delta_B.append(A)\r\n",
        "       l = l-1\r\n",
        "\r\n",
        "    delta_W.reverse()\r\n",
        "    delta_B.reverse()\r\n",
        "    return [delta_W,delta_B]\r\n",
        "\r\n",
        "    \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wyYkWzOyyOT"
      },
      "source": [
        "def upd_lst(lst1,lst2,operation,eeta):\r\n",
        "   l = []\r\n",
        "\r\n",
        "   for (x,y) in zip(lst1,lst2):\r\n",
        "     if(operation == '+'):\r\n",
        "       l.append(x+(eeta*y))\r\n",
        "     else:\r\n",
        "       l.append(x-(eeta*y))\r\n",
        "\r\n",
        "   return l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP-FV-Ijsclo"
      },
      "source": [
        "def sgd(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative):\r\n",
        "\r\n",
        "     for i in range(num_epochs): \r\n",
        "          cnt = 0\r\n",
        "          cum_W = []\r\n",
        "          cum_B = []\r\n",
        "          for j in range(X.shape[0]):\r\n",
        "\r\n",
        "             cnt = cnt + 1\r\n",
        "             r = feed_forward(X[j],activation_function,weights,biases)\r\n",
        "             a = r[0]\r\n",
        "             h = r[1]\r\n",
        "             yout = r[2]\r\n",
        "             gradients = caluculate_gradients(Y[j],yout,h,a,weights)\r\n",
        "             cum_W = upd_lst(cum_W,gradients[0],'+',1)\r\n",
        "             cum_B = upd_lst(cum_B,gradients[1],'+',1)\r\n",
        "             if(cnt == batch_size): \r\n",
        "                weights = upd_lst(weights,cum_W,'-',(learning_rate*(1-weight_decay))/batch_size)\r\n",
        "                biases = upd_lst(biases,cum_B,'-',(learning_rate*(1-weight_decay))/batch_size)\r\n",
        "                cum_W = []\r\n",
        "                cum_B = []\r\n",
        "                cnt = 0\r\n",
        "      \r\n",
        "     return [weights,biases]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuklB5S78jQy"
      },
      "source": [
        "def backpropagation(X,Y,num_epochs,num_hidden_layers,sz_hidden_layer,weight_decay,learning_rate,optimizer,batch_size,weight_initialisation,activation_function):\r\n",
        "    \r\n",
        "    \r\n",
        "    #do the weights Initialisation here based on weight_initialisation,num_hidden_layers,sz_each_hidden_layer\r\n",
        "    \r\n",
        "    weights = [0]  #dummy value\r\n",
        "    biases =  [0]  #dummy value\r\n",
        "\r\n",
        "    if(weight_initialisation == 'random'):\r\n",
        "          \r\n",
        "          num_classes = len(np.unique(Y))\r\n",
        "          i = len(X[0].flatten())\r\n",
        "          a = -100\r\n",
        "          b = 100\r\n",
        "\r\n",
        "          for j in range(num_hidden_layers-1):\r\n",
        "              weights.append((200)*np.random.rand(sz_hidden_layer,i)-100)\r\n",
        "              biases.append((200)*np.random.rand(sz_hidden_layer)-100)\r\n",
        "              i = sz_hidden_layer\r\n",
        "\r\n",
        "          weights.append((200)*np.random.rand(num_classes,i)-100)\r\n",
        "          biases.append((200)*np.random.rand(num_classess)-100)\r\n",
        "          \r\n",
        "    else:\r\n",
        "         print(\"Please Implement Xavier haha !!\")  \r\n",
        "    if(activation_function=='sigmoid'):\r\n",
        "      activation_function=sigmoid\r\n",
        "      act_fn_derivative = sigmoid_grad\r\n",
        "    elif(activation_function=='tanh'):\r\n",
        "      activation_function=tanh\r\n",
        "      act_fn_derivative = tanh_grad\r\n",
        "    elif(activation_function=='ReLU'):\r\n",
        "      activation_function= relu\r\n",
        "      act_fn_derivative = relu_grad\r\n",
        "        \r\n",
        "\r\n",
        "    if(optimizer == 'sgd'):\r\n",
        "       return sgd(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "      \r\n",
        "    if(optimizer == 'momentum'): \r\n",
        "       return momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    if(optimizer == 'nesterov'): \r\n",
        "       return nesterov(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    if(optimizer == 'rmsprop'):\r\n",
        "       return rmsprop(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    if(optimizer == 'adam'):\r\n",
        "      return adam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "\r\n",
        "    if(optimizer == 'nadam'):\r\n",
        "      return nadam(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,act_fn_derivative)\r\n",
        "  \r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gsjii9ICVT3"
      },
      "source": [
        "def momentum(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "\r\n",
        "  #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "  weights_grad_tot=[]\r\n",
        "  for w in weights:\r\n",
        "    weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "  biases_grad_tot=[]\r\n",
        "  for b in biases:\r\n",
        "    biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "\r\n",
        "  #Initialisation of update_weights\r\n",
        "  update_weights=weights_grad_tot\r\n",
        "  update_biases=biases_grad_tot\r\n",
        "\r\n",
        "  for i in range(num_epochs):\r\n",
        "    num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "    Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "    num_data_point_done=0\r\n",
        "    for x,y,y_pred in zip(X,Y,Y_pred):    \r\n",
        "      [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,weights,act_fn_derivative)\r\n",
        "      weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "      biases_grad_tot = biases_grad_tot + biases_grad      \r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        update_weights = update_weights + gamma*update_weights + learning_rate * weights_grad_tot\r\n",
        "        update_biases = update_biases + gamma*update_weight + learning_rate * biases_grad_tot\r\n",
        "        weights=weights-update_weights\r\n",
        "        biases=biases - update_biases\r\n",
        "        num_data_point_done=0\r\n",
        "  \r\n",
        "def nesterov(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "  \r\n",
        "  #Initialisation of weights_grad_tot, biases_grad_tot\r\n",
        "  weights_grad_tot=[]\r\n",
        "  for w in weights:\r\n",
        "    weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "  biases_grad_tot=[]\r\n",
        "  for b in biases:\r\n",
        "    biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "  #Initial value of w_head\r\n",
        "  w_ahead = weights\r\n",
        "\r\n",
        "  #Initialisation of update_weights\r\n",
        "  update_weights=weights_grad_tot\r\n",
        "  update_biases=biases_grad_tot\r\n",
        "\r\n",
        "  for i in range(num_epochs):\r\n",
        "    num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "    Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "    num_data_point_done=0\r\n",
        "    for x,y,y_pred in zip(X,Y,Y_pred):          \r\n",
        "      [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,w_ahead,act_fn_derivative)\r\n",
        "      weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "      biases_grad_tot = biases_grad_tot + biases_grad             \r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        w_ahead = weights -  update_weights\r\n",
        "        b_ahead = biases -  update_biases\r\n",
        "        update_weights = update_weights + gamma*update_weights + learning_rate * weights_grad_tot\r\n",
        "        update_biases = update_biases + gamma*update_weight + learning_rate * biases_grad_tot    \r\n",
        "        weights=weights-update_weights\r\n",
        "        biases=biases - update_biases\r\n",
        "        num_data_point_done=0\r\n",
        "\r\n",
        "  \r\n",
        "def adagrad(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "  # w_ahead= weights\r\n",
        "  # b_ahead=biases\r\n",
        "  for i in range(num_epochs):\r\n",
        "    num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "    Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "    weights_grad_tot=[]\r\n",
        "    for w in weights:\r\n",
        "      weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "    biases_grad_tot=[]\r\n",
        "    for b in biases:\r\n",
        "      biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "    num_data_point_done=0\r\n",
        "    v_weight = 0 #Intitial value of 0\r\n",
        "    v_bias=0\r\n",
        "    epsilon = 0 #Figure out what is epsilon\r\n",
        "    for x,y,y_pred in zip(X,Y,Y_pred):    \r\n",
        "      [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,weights,act_fn_derivative)\r\n",
        "      weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "      biases_grad_tot = biases_grad_tot + biases_grad\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        v_weight = v_weight + (weights_grad_total)**2\r\n",
        "        v_bias = v_bias + (biases_grad_total)**2\r\n",
        "        v_weight_dum = learning_rate/np.sqrt(v_weight+epsilon)\r\n",
        "        v_bias_dum = learning_rate/np.sqrt(v_bias+epsilon)\r\n",
        "        weights=weights- v_weight_dum * weights_grad_tot\r\n",
        "        biases=biases - v_bias_dum * bias_grad_tot\r\n",
        "        num_data_point_done=0\r\n",
        "    \r\n",
        "def rmsprop(X,Y,num_epochs,weight_decay,learning_rate,batch_size,weights,biases,activation_function,gamma,act_fn_derivative):\r\n",
        "  #w_ahead=w_{t} - gamma. update_{t-1}\r\n",
        "  #update{t} = gamma . update_{t-1} + learning_rate * grad(w_lookahead)\r\n",
        "  # w_ahead= weights\r\n",
        "  # b_ahead=biases\r\n",
        "  for i in range(num_epochs):\r\n",
        "    num_hidden_layers=biases.size()[0] #Biases is a list of 1D arrays. Thus, the number of hidden layers is given by the size of this list\r\n",
        "    Y_pred=feed_forward(X,activation_function,weights,biases) #Find prediction for each data instance of the data set\r\n",
        "    weights_grad_tot=[]\r\n",
        "    for w in weights:\r\n",
        "      weights_grad_tot.append(np.zeroes(w.size()))\r\n",
        "    biases_grad_tot=[]\r\n",
        "    for b in biases:\r\n",
        "      biases_grad_tot.append(np.zeroes(b.size()))\r\n",
        "    num_data_point_done=0\r\n",
        "    v_weight = 0 #Intitial value of 0\r\n",
        "    v_bias=0\r\n",
        "    epsilon = 0 #Figure out what is epsilon\r\n",
        "    beta=0\r\n",
        "    for x,y,y_pred in zip(X,Y,Y_pred):    \r\n",
        "      [weights_grad,biases_grad] =  calculate_gradient(y,y_pred,h,a,weights,act_fn_derivative)\r\n",
        "      weights_grad_tot = weights_grad_tot+weights_grad\r\n",
        "      biases_grad_tot = biases_grad_tot + biases_grad\r\n",
        "      num_data_point_done=num_data_point_done+1\r\n",
        "      if(num_data_point_done==batch_size):\r\n",
        "        v_weight = beta*v_weight + (1-beta)*(weights_grad_total)**2\r\n",
        "        v_bias = beta*v_bias + (1-beta)*(biases_grad_total)**2\r\n",
        "        v_weight_dum = learning_rate/np.sqrt(v_weight+epsilon)\r\n",
        "        v_bias_dum = learning_rate/np.sqrt(v_bias+epsilon)\r\n",
        "        weights=weights- v_weight_dum * weights_grad_tot\r\n",
        "        biases=biases - v_bias_dum * bias_grad_tot\r\n",
        "        num_data_point_done=0\r\n",
        "      \r\n",
        "      \r\n",
        "  \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDmQzYnqwokO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}